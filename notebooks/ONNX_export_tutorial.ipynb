{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brevitas requires Python 3.7+ and PyTorch 1.5.1+ and can be installed from PyPI with `pip install brevitas`. \n",
    "\n",
    "For this notebook, you will also need to install ONNX, onnxruntime, and onnxoptimizer.\n",
    "For this tutorial, PyTorch 1.8.1+ is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this notebook is to show how to use Brevitas to export your models in the two standards currently supported by ONNX for quantized models: QCDQ and QOps (i.e., QLinearConv, QLinearMatMul).\n",
    "Once exported, these models can be run using onnxruntime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuantizeLinear-Clip-DeQuantizeLinear (QCDQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In QCDQ export, before each quantized operation, two (or three, in case of clipping) extra ONNX nodes are added:\n",
    "- QuantizeLinear: Takes as input a FP tensor, and quantizes it with a given zero-point and scale factor. It returns an INT8 tensor.\n",
    "- Clip (Optional): Takes as input an INT8 tensor, and, given a max/min value, restricts its range.\n",
    "- DeQuantizeLinear: Takes as input an INT8 tensor, and converts it to its FP correspondant with a given zero-point and scale factor.\n",
    "\n",
    "There are several implications associated with this set of operations:\n",
    "- It is not possible to quantize with a bitwidth higher than 8. Although DequantizeLinear supports both INT8 and INT32 as input, QuantizeLinear will alwyas output INT8 (either signed or unsigned).\n",
    "- Using only QuantizeLinear and DequantizeLinear, it is possible only to quantize at 8 bit (signed or unsigned).\n",
    "- The addition of the Clip function between Quantize and DeQuantize, allows to quantize a tensor to bitwidth < 8. This is done by Clipping the INT8 tensor coming out of the QuantizeLinear node with the max/min values of the desired bitwidth (e.g., for unsigned 3 bit, min_val = 0 and max_val = 7).\n",
    "- It is possible to perform per-channel and per-tensor quantization (only supported with ONNX Opset >=13).\n",
    "\n",
    "We will go through all these cases with some examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will look at `brevitas.nn.QuantLinear`, a quantized alternative to `torch.nn.Linear`. Similar considerations can also be used for  `QuantConv1d`, `QuantConv2d`,  `QuantConvTranspose1d` and `QuantConvTranspose2d`.\n",
    "\n",
    "Brevitas offers several API to export Pytorch modules into several different formats, all sharing the same interface.\n",
    "The three required arguments are:\n",
    "- The pytorch model to export\n",
    "- A representative input \n",
    "- The path where to save the exported model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "import time\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def show_netron(model_path, port):\n",
    "    time.sleep(3.)\n",
    "    netron.start(model_path, address=(\"localhost\", port), browse=False)\n",
    "    return IFrame(src=f\"http://localhost:{port}/\", width=\"100%\", height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.nn as qnn\n",
    "import torch\n",
    "from brevitas.export import export_standard_qcdq_onnx\n",
    "\n",
    "IN_CH = 3\n",
    "OUT_CH = 128\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "linear = qnn.QuantLinear(IN_CH, OUT_CH, bias=True)\n",
    "inp = torch.randn(BATCH_SIZE, IN_CH)\n",
    "path = 'simple_model.onnx'\n",
    "\n",
    "exported_model = export_standard_qcdq_onnx(linear, args=inp, export_path=path, opset_version=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://localhost:8082\n",
      "Serving 'simple_model.onnx' at http://localhost:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8d980c2150>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_netron(path, 8082)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the exported onnx, in this case only the weights are quantized, and they go through a Quantize/Dequantize Linear before being used for the convolution. Moreover, there is a clipping operation, setting the max/min val for the Tensor at Â±127.\n",
    "\n",
    "This is because in Brevitas, by defult, quantized layers (but not activation) have the option `narrow_range=True`. \n",
    "This option, in case of signed quantization, makes sure that the maximum and minimum number that can be represented are the same (otherwise, the minimum integer would be -128).\n",
    "\n",
    "\n",
    "The input and bias remains in floating point, but inn QCDQ export, this is not a problem since even the weights, that are quantized at 8 bit, are re-converted to FP before passed as input to the Linear node.\n",
    "\n",
    "It is important to know that ONNX will automatically decide what nodes to use for each computation, based on the characteristics of the inputs and outputs. For example, if the parameter `bias` is set to False, the `Gemm` node would be replaced by a LinearMatMul. This has no immediate effect on the export flow, the quantization, or the numerical correctness of the resulting model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Complete\" Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach can be used with entire Pytorch models, rather than single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = qnn.QuantLinear(IN_CH, OUT_CH, bias=True, return_quant_tensor=True, weight_scaling_per_output_channel=True)\n",
    "        self.act = qnn.QuantReLU()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inp = self.linear(inp)\n",
    "        inp = self.act(inp)\n",
    "        return inp\n",
    "\n",
    "model = Model()\n",
    "inp = torch.randn(BATCH_SIZE, IN_CH)\n",
    "path = 'simple_model.onnx'\n",
    "\n",
    "exported_model = export_standard_qcdq_onnx(model, args=inp, export_path=path, opset_version=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://localhost:8082\n",
      "Serving 'simple_model.onnx' at http://localhost:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8d52b5ed90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_netron(path, 8082)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not specify the argument `output_quant` in our QuantLinear layer, thus the output of the layer will be passed directly to the ReLU function without any intermediate re-quantization step. \n",
    "\n",
    "Furthermore, we have defined a per-channel quantization, so the scale factor will be a Tensor rather than a scalar (ONNX opset >= 13 is required for this).\n",
    "\n",
    "Finally, since we are using a QuantReLU with default initialization, the output is requantized as an UInt8 Tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The C in QCDQ (Bitwidth <= 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, Brevitas export expands on the basic QDQ format by adding the Clipping operation.\n",
    "\n",
    "This operations is inserted between the QuantizeLinear and DequantizeLinear node, and thus operates on integers.\n",
    "\n",
    "Normally, using only the QDQ format, it would be impossible to export models quantize with less than 8 bit.\n",
    "\n",
    "In Brevitas however, if a quantized layer with bitwidth <= 8 is exported, the Clip node will be automatically inserted, and it will perform a new `saturate` operation, computing the new max and min values based on the particular type of quantized performed (i.e., signed vs unsigned, narrow range vs no narrow range, etc.).\n",
    "\n",
    "Even though the Tensor data type will still be a Int8 or UInt8, in practical term the tensor will represent the desired bitwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = qnn.QuantLinear(IN_CH, OUT_CH, bias=True, return_quant_tensor=True, weight_bit_width=3)\n",
    "        self.act = qnn.QuantReLU(bit_width=4)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inp = self.linear(inp)\n",
    "        inp = self.act(inp)\n",
    "        return inp\n",
    "\n",
    "model = Model()\n",
    "model.eval()\n",
    "\n",
    "inp = torch.randn(BATCH_SIZE, IN_CH)\n",
    "path = 'simple_model.onnx'\n",
    "\n",
    "exported_model = export_standard_qcdq_onnx(model, args=inp, export_path=path, opset_version=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://localhost:8082\n",
      "Serving 'simple_model.onnx' at http://localhost:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8d52b91bd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_netron(path, 8082)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the generated ONNX, the weights of the Linear layer are clipped between -3 and 3, considering that we are performing a signed, 3 bit quantization, with `narrow_range=True`. \n",
    "\n",
    "Similarly, the output of the QuantReLU is clipped between 0 and 15, since in this case we are doing an unsigned 4 bit quantization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QOps Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another supported style for exporting quantized operation in ONNX is represented by QOperations. \n",
    "\n",
    "Compared to QCDQ, where it is possible to re-use standard floating point layers (e.g., Linear or Conv2d) precedeed by QCDQ nodes, in this case the entire layer is replaced with its quantized counterpart. \n",
    "\n",
    "Opposite to what happens with QCDQ, all elements of the computation in this case have to be quantized: Input, Weight, Bias (if present), and Output tensors.\n",
    "\n",
    "This introduces some contraints on how we define our quantized layers through Brevitas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://localhost:8082\n",
      "Serving 'simple_model.onnx' at http://localhost:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8d5044e950>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
    "from brevitas.export import export_standard_qop_onnx\n",
    "\n",
    "IN_CH = 3\n",
    "IMG_SIZE = 128\n",
    "OUT_CH = 128\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.act = qnn.QuantIdentity(return_quant_tensor=True)\n",
    "        self.linear = qnn.QuantConv2d(IN_CH, OUT_CH, kernel_size=3, bias=False, \n",
    "                                      weight_bit_width=4,\n",
    "                                      output_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inp = self.act(inp)\n",
    "        inp = self.linear(inp)\n",
    "        return inp\n",
    "\n",
    "inp = torch.randn(BATCH_SIZE, IN_CH, IMG_SIZE, IMG_SIZE)\n",
    "model = Model() \n",
    "model.eval()\n",
    "\n",
    "\n",
    "export_standard_qop_onnx(\n",
    "    model.cpu(),\n",
    "    input_t=inp,\n",
    "    export_path=\"simple_model.onnx\",\n",
    "    opset_version=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "show_netron(\"simple_model.onnx\", 8082)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we need to make sure that our input to QuantLinear is quantized. Using the approach shown above, Brevitas will add a QuantizeLinear node (but not followed by a DeQuantizeLinear one). \n",
    "\n",
    "Moreover, our `qnn.QuantLinear` layer has to specify how to re-quantize the output (in this case, with `Int8ActPerTensorFloat`), otherwise an error will be raised during export-time.\n",
    "\n",
    "Similarly, if the bias is present, this has to be quantized or an error will be raised.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clipping in QOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when using QLinearConv and QLinearMatMul, it is still possible to represent bitwidth <8 through the use of clipping.\n",
    "\n",
    "However in this case the Clipping operation won't be captured in the exported ONNX graph. Instead, it will be performed at export-time, and the clipped tensor will be exported in the ONNX graph.\n",
    "\n",
    "Examining the last exported model, it is possible to see that the weight tensor, even though has Int8 has type, has a max/min values between [-8, 8], given that it is quantized at 4 bit with narrow_range set to True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX RUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QCDQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for QCDQ we are only using standard ONNX operation, it is possible to run the exported model using ONNX runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 16:22:03.824044539 [W:onnxruntime:, graph.cc:1271 Graph] Initializer linear.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2022-12-09 16:22:03.824084348 [W:onnxruntime:, graph.cc:1271 Graph] Initializer linear.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = qnn.QuantLinear(IN_CH, OUT_CH, bias=True, return_quant_tensor=True, weight_bit_width=3)\n",
    "        self.act = qnn.QuantReLU(bit_width=4)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inp = self.linear(inp)\n",
    "        inp = self.act(inp)\n",
    "        return inp\n",
    "\n",
    "model = Model()\n",
    "model.eval()\n",
    "inp = torch.randn(BATCH_SIZE, IN_CH)\n",
    "path = 'simple_model.onnx'\n",
    "\n",
    "exported_model = export_standard_qcdq_onnx(model, args=inp, export_path=path, opset_version=13)\n",
    "\n",
    "sess_opt = ort.SessionOptions()\n",
    "sess = ort.InferenceSession(path, sess_opt)\n",
    "input_name = sess.get_inputs()[0].name\n",
    "pred_onx = sess.run(None, {input_name: inp.numpy()})[0]\n",
    "\n",
    "\n",
    "out_brevitas = model(inp)\n",
    "out_ort = torch.tensor(pred_onx)\n",
    "\n",
    "assert torch.allclose(out_brevitas, out_ort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QGEMM vs GEMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, ONNX will decide the appropriate kernel to execute for each operation based on the characteristics of the input and output tensors (e.g., Gemm vs MatMul).\n",
    "\n",
    "Similarly, when using ONNX runtime, we observed a similar behavior.\n",
    "\n",
    "In very particular scenarios, we noticed that, even though in QCDQ all operations between tensors should be in FP, ONNX runtime calls quantized kernels, suggesting that the DequantizeLinear node is moved and fused in the computation.\n",
    "\n",
    "This seems to happen only when using a Quantized Linear layer, with the following requirements:\n",
    "- Input, Weight, Bias, and Output tensors must be quantized;\n",
    "- Bias tensor must be present, and quantized with bitwidth > 8;\n",
    "- The output of the QuantLinear must be re-quantized;\n",
    "- The output bitwidth must be equal to 8;\n",
    "- The input bitwidth must be equal to 8;\n",
    "- The weights bitwidth can be <= 8;\n",
    "- The weights can be quantized per-tesor or per-channel;\n",
    "- `return_quant_tensor` must be True.\n",
    "\n",
    "We did not observe a similar behavior for other operations such as `QuantConvNd`.\n",
    "\n",
    "An example of a layer that will match this definition is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.quant.scaled_int import Int16Bias\n",
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
    "\n",
    "qgemm_ort = qnn.QuantLinear(IN_CH, OUT_CH,\n",
    "                            weight_bit_width=5,\n",
    "                            input_quant=Int8ActPerTensorFloat,\n",
    "                            output_quant=Int8ActPerTensorFloat,\n",
    "                            bias=True, bias_quant=Int16Bias,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we have not found a way to determine what kernels onnxruntime uses for computation.\n",
    "\n",
    "We found out about this behavior in the our development process, and confirmed it by re-compiling from source the onnxruntime library.\n",
    "This behavior has been observed by building from source onnxruntime v1.13.1, and ONNX v1.12.\n",
    "It might be needed to set the following environmental variable `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` for this to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the QCDQ case, also in this case we are using only standard ONNX operations, thus we can use onnxruntime for executing our exported models. \n",
    "\n",
    "The main difference is that all operations happen between quantized tensor, thus we should expect to get Int8 or UInt8 tensors from our execution, rather than their floating point versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.act = qnn.QuantIdentity(return_quant_tensor=True)\n",
    "        self.conv = qnn.QuantConv2d(IN_CH, OUT_CH, kernel_size=3, bias=False, \n",
    "                                      weight_bit_width=4,\n",
    "                                      \n",
    "                                      output_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inp = self.act(inp)\n",
    "        inp = self.conv(inp)\n",
    "        return inp\n",
    "\n",
    "model = Model()\n",
    "model.eval()\n",
    "inp = torch.randn(BATCH_SIZE, IN_CH, IMG_SIZE, IMG_SIZE)\n",
    "path = 'simple_model.onnx'\n",
    "\n",
    "exported_model = export_standard_qop_onnx(model, args=inp, export_path=path, opset_version=13)\n",
    "\n",
    "sess_opt = ort.SessionOptions()\n",
    "sess = ort.InferenceSession(path, sess_opt)\n",
    "input_name = sess.get_inputs()[0].name\n",
    "pred_onx = sess.run(None, {input_name: inp.numpy()})[0]\n",
    "\n",
    "\n",
    "out_brevitas = model(inp).int()\n",
    "out_ort = torch.tensor(pred_onx).int()\n",
    "\n",
    "assert torch.allclose(out_brevitas, out_ort, atol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, before comparing the results, we first convert floating point input to its integer representation, using the scale factor and zero point present in the QuantTensor.\n",
    "\n",
    "Due to differences in how the computation is performed, it might happen the two results are slightly different (since Brevitas uses a style closer to QCDQ, rather than operating between integers), thus we added a slighly higher tolerance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
