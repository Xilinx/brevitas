{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized RNNs and LSTMs\n",
    "\n",
    "With version 0.8, Brevitas introduces support for quantized recurrent layers through `QuantRNN` and `QuantLSTM`. As with other Brevitas quantized layers, `QuantRNN` and `QuantLSTM` can be used as drop-in replacement for their floating-point variants, but they also go further and support some additional structural recurrent options not found in upstream PyTorch. Similarly to other quantized layers, both `QuantRNN` and `QuantLSTM` can take in different quantizers for different tensors involved in their computation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuantRNN\n",
    "\n",
    "We start by looking at `QuantRNN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "    def __init__(\n",
       "            self,\n",
       "            input_size: int,\n",
       "            hidden_size: int,\n",
       "            num_layers: int = 1,\n",
       "            nonlinearity: str = 'tanh',\n",
       "            bias: bool = True,\n",
       "            batch_first: bool = False,\n",
       "            bidirectional: bool = False,\n",
       "            weight_quant = Int8WeightPerTensorFloat,\n",
       "            bias_quant = Int32Bias,\n",
       "            io_quant = Int8ActPerTensorFloat,\n",
       "            gate_acc_quant = Int8ActPerTensorFloat,\n",
       "            shared_input_hidden_weights = False,\n",
       "            return_quant_tensor: bool = False,\n",
       "            **kwargs):\n",
       "        super(QuantRNN, self).__init__(\n",
       "            layer_impl=_QuantRNNLayer,\n",
       "            input_size=input_size,\n",
       "            hidden_size=hidden_size,\n",
       "            num_layers=num_layers,\n",
       "            nonlinearity=nonlinearity,\n",
       "            bias=bias,\n",
       "            batch_first=batch_first,\n",
       "            bidirectional=bidirectional,\n",
       "            weight_quant=weight_quant,\n",
       "            bias_quant=bias_quant,\n",
       "            io_quant=io_quant,\n",
       "            gate_acc_quant=gate_acc_quant,\n",
       "            shared_input_hidden_weights=shared_input_hidden_weights,\n",
       "            return_quant_tensor=return_quant_tensor,\n",
       "            **kwargs)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import inspect\n",
    "from brevitas.nn import QuantRNN\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def pretty_print_source(source):\n",
    "    display(Markdown('```python\\n' + source + '\\n```'))\n",
    "    \n",
    "source = inspect.getsource(QuantRNN.__init__)  \n",
    "pretty_print_source(source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QuantRNN` supports all arguments of `torch.nn.RNN`, plus it exposes four different quantizers: `weight_quant` controls quantization of the weight tensor, `bias_quant` controls quantization of the bias, `io_quant` controls quantization of the input/output, and `gate_acc_quant` controls quantization of the output of the gate, before the nonlinearity is applied. \n",
    "\n",
    "Compared to other layers like `QuantLinear`, a couple of things can be observed. First, input and output quantization are fused together into `io_quant`. This is because of the recurrent structure of RNN layers, where the output is fed back as input. Second, all quantizers are already set by default. This is different from a layer like `QuantLinear`, where only `weight_quant` has a default quantizer.\n",
    "\n",
    "As with `torch.nn.RNN`, `QuantRNN` defines a stack of potentially multiple layers, controlled by setting `num_layers`, that can be set to bidirectional with `bidirectional=True`. Internally, `QuantRNN` is organized into a two level nesting of `ModuleList`, one for the different layer(s), and one for the direction(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alessand\\documents\\brevitas\\src\\brevitas\\nn\\mixin\\base.py:112: UserWarning: Keyword arguments are being passed but they not being used.\n",
      "  warn('Keyword arguments are being passed but they not being used.')\n"
     ]
    }
   ],
   "source": [
    "def rnn_sublayer(module, sublayer_number, right_to_left_direction):\n",
    "    return module.layers[sublayer_number][1 if right_to_left_direction else 0]\n",
    "\n",
    "quant_rnn = QuantRNN(input_size=10, hidden_size=20, num_layers=2, bidirectional=True)\n",
    "quant_rnn_0_left_to_right = rnn_sublayer(quant_rnn, sublayer_number=0, right_to_left_direction=False)\n",
    "quant_rnn_0_right_to_left = rnn_sublayer(quant_rnn, sublayer_number=0, right_to_left_direction=True)\n",
    "quant_rnn_1_left_to_right = rnn_sublayer(quant_rnn, sublayer_number=1, right_to_left_direction=False)\n",
    "quant_rnn_1_right_to_left = rnn_sublayer(quant_rnn, sublayer_number=1, right_to_left_direction=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `num_layers > 1` and/or `bidirectional=True` has different implications on different quantizers. For `weight_quant`, `gate_acc_quant` and `bias_quant`, the same quantizer *definition* is shared among different layers/directions, but each layer/direction is allocated its own instance of the quantizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_rnn_0_left_to_right.gate_params.input_weight.weight_quant is quant_rnn_1_right_to_left.gate_params.input_weight.weight_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_rnn_0_left_to_right.cell.gate_acc_quant is quant_rnn_1_right_to_left.cell.gate_acc_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_rnn_0_left_to_right.gate_params.bias_quant is quant_rnn_1_right_to_left.gate_params.bias_quant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, for `io_quant` the same *instance* is gonna be shared among all layers and directions. This is to make sure that input/output tensors that are internally concatenated together share the same quantization scale/zero-point/bitwidth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_rnn_0_left_to_right.io_quant is quant_rnn_1_right_to_left.io_quant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `QuantRNN` supports an additional flag, `shared_input_hidden_weights`. This allows, whenever `bidirectional=True`, to share the input-to-hidden weights among the two directions, an optimization introduced first by DeepSpeech back in the day to save on the number of parameters, with minimal impact on the quality of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights for single direction QuantRNN: 600\n",
      "Number of weights for bidirectional QuantRNN: 1200\n",
      "Number of weights for bidirectional QuantRNN with shared input-hidden weights: 1000\n"
     ]
    }
   ],
   "source": [
    "from brevitas.nn import QuantRNN\n",
    "\n",
    "def count_weights(model):\n",
    "    return sum(p.numel() for n, p in model.named_parameters() if 'weight' in n)\n",
    "\n",
    "quant_rnn_single_direction = QuantRNN(input_size=10, hidden_size=20, bidirectional=False, shared_input_hidden_weights=False)\n",
    "quant_rnn_bidirectional = QuantRNN(input_size=10, hidden_size=20, bidirectional=True, shared_input_hidden_weights=False)\n",
    "quant_rnn_bidirectional_shared_input_hidden = QuantRNN(input_size=10, hidden_size=20, bidirectional=True, shared_input_hidden_weights=True)\n",
    "\n",
    "print(f\"Number of weights for single direction QuantRNN: {count_weights(quant_rnn_single_direction)}\")\n",
    "print(f\"Number of weights for bidirectional QuantRNN: {count_weights(quant_rnn_bidirectional)}\")\n",
    "print(f\"Number of weights for bidirectional QuantRNN with shared input-hidden weights: {count_weights(quant_rnn_bidirectional_shared_input_hidden)}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other Brevitas layers, it's possible to directly modify a quantizer by passing keyword arguments with a matching prefix. For example, to set 4b per-channel weights and 6b io quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-hidden weight bit-width: 4.0\n",
      "Hidden-hidden weight bit-width: 4.0\n",
      "I/O quant bit-width: 6.0\n",
      "Input-hidden weight scale: tensor([[0.0318],\n",
      "        [0.0303],\n",
      "        [0.0316],\n",
      "        [0.0296],\n",
      "        [0.0316],\n",
      "        [0.0310],\n",
      "        [0.0313],\n",
      "        [0.0319],\n",
      "        [0.0319],\n",
      "        [0.0319],\n",
      "        [0.0319],\n",
      "        [0.0315],\n",
      "        [0.0315],\n",
      "        [0.0314],\n",
      "        [0.0319],\n",
      "        [0.0307],\n",
      "        [0.0303],\n",
      "        [0.0319],\n",
      "        [0.0282],\n",
      "        [0.0286]], grad_fn=<DivBackward0>)\n",
      "Hidden-hidden weight scale: tensor([[0.0318],\n",
      "        [0.0303],\n",
      "        [0.0316],\n",
      "        [0.0296],\n",
      "        [0.0316],\n",
      "        [0.0310],\n",
      "        [0.0313],\n",
      "        [0.0319],\n",
      "        [0.0319],\n",
      "        [0.0319],\n",
      "        [0.0319],\n",
      "        [0.0315],\n",
      "        [0.0315],\n",
      "        [0.0314],\n",
      "        [0.0319],\n",
      "        [0.0307],\n",
      "        [0.0303],\n",
      "        [0.0319],\n",
      "        [0.0282],\n",
      "        [0.0286]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "quant_rnn_4b = QuantRNN(input_size=10, hidden_size=20, weight_bit_width=4, weight_scaling_per_output_channel=True, io_bit_width=6)\n",
    "quant_rnn_4b_0_left_to_right = rnn_sublayer(quant_rnn_4b, sublayer_number=0, right_to_left_direction=False)\n",
    "\n",
    "input_hidden_weight = quant_rnn_4b_0_left_to_right.gate_params.input_weight.quant_weight()\n",
    "hidden_hidden_weight = quant_rnn_4b_0_left_to_right.gate_params.hidden_weight.quant_weight()\n",
    "\n",
    "print(f\"Input-hidden weight bit-width: {input_hidden_weight.bit_width}\")\n",
    "print(f\"Hidden-hidden weight bit-width: {hidden_hidden_weight.bit_width}\")\n",
    "print(f\"I/O quant bit-width: {quant_rnn_4b_0_left_to_right.io_quant.bit_width()}\")\n",
    "print(f\"Input-hidden weight scale: {input_hidden_weight.scale}\")\n",
    "print(f\"Hidden-hidden weight scale: {hidden_hidden_weight.scale}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QuantRNN` follows the same `forward` interface of `torch.nn.RNN`, with a couple of exceptions. Packed variable length inputs are currently not supported, and unbatched inputs are not supported. \n",
    "Other than that, everything else is the same. \n",
    "\n",
    "Inputs are expected to have shape `(batch, sequence, features)` for `batch_first=False`, or `(sequence, batch, features)` for `batch_first=True`. The layer returns a tuple with `(outputs, hidden_states)`, where `outputs` has shape `(sequence, batch, hidden_size * num_directions)` with `num_directions=2` when `bidirectional=True`, for `batch_first=False`, or `(batch, sequence, hidden_size * num_directions)` for `batch_first=True`, while `hidden_states` has shape `(num_directions * num_layers, batch, hidden_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([2, 5, 20])\n",
      "Hidden states size: torch.Size([1, 2, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantRNN\n",
    "\n",
    "quant_rnn = QuantRNN(input_size=10, hidden_size=20, batch_first=True)\n",
    "outputs, hidden_states = quant_rnn(torch.randn(2, 5, 10))\n",
    "print(f\"Output size: {outputs.shape}\")\n",
    "print(f\"Hidden states size: {hidden_states.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other quantized layers, it's possible to return a `QuantTensor` with `return_quant_tensor=True`. As a reminder, a `QuantTensor` is just a data structure that captures the quantization metadata associated with a quantized tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alessand\\documents\\brevitas\\src\\brevitas\\nn\\mixin\\base.py:343: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\python_arg_parser.cpp:354.)\n",
      "  return torch.cat(outputs, dim=seq_dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(QuantTensor(value=tensor([[[ 0.0973,  0.6080, -0.4378,  0.0304, -0.6992, -0.3587,  0.6141,\n",
       "            0.4803, -0.3222, -0.1277, -0.3587,  0.4621, -0.2858, -0.0182,\n",
       "           -0.5411, -0.2006,  0.4925,  0.0547,  0.1642, -0.0243],\n",
       "          [ 0.2257,  0.3095,  0.0709,  0.0580, -0.3353, -0.1096,  0.0064,\n",
       "            0.6191,  0.0451, -0.3740,  0.1741,  0.0709, -0.4450, -0.0451,\n",
       "           -0.3418,  0.1354,  0.2451,  0.3740, -0.3353,  0.2257],\n",
       "          [ 0.0205,  0.0307,  0.0000,  0.3323,  0.6289, -0.2761,  0.0716,\n",
       "            0.1125,  0.5982, -0.4295,  0.3119,  0.3886,  0.2608,  0.1278,\n",
       "            0.5062,  0.1687,  0.1892, -0.4857, -0.2250,  0.5164],\n",
       "          [ 0.3314,  0.3779, -0.3895,  0.4419, -0.1919, -0.4302,  0.0988,\n",
       "           -0.1512, -0.0872, -0.4012,  0.1105,  0.0465, -0.0291,  0.2267,\n",
       "           -0.3605,  0.6221,  0.1105, -0.1047, -0.0988, -0.2442],\n",
       "          [-0.0868,  0.6122,  0.1350,  0.0289, -0.1446,  0.1060,  0.1928,\n",
       "            0.3712, -0.3712, -0.1976, -0.0675,  0.4772, -0.1205, -0.3953,\n",
       "            0.0096, -0.2603,  0.2169,  0.1832,  0.3230,  0.0675]],\n",
       " \n",
       "         [[ 0.2493, -0.3040,  0.3040,  0.2432, -0.2432, -0.3891, -0.3648,\n",
       "            0.7722, -0.3587,  0.1763,  0.0912,  0.6202,  0.5776, -0.4621,\n",
       "           -0.7661,  0.4134, -0.4986,  0.7722,  0.7418, -0.3830],\n",
       "          [-0.5352, -0.3611,  0.3160,  0.4966,  0.4450,  0.2708, -0.6191,\n",
       "           -0.4256, -0.7158, -0.3095,  0.2257, -0.8254,  0.5094, -0.7545,\n",
       "           -0.2515, -0.0903,  0.0387,  0.5933, -0.6578, -0.3095],\n",
       "          [ 0.5624, -0.3784,  0.2966,  0.0409,  0.4602,  0.5522,  0.1636,\n",
       "           -0.3119,  0.6340,  0.0614, -0.2403, -0.0665,  0.6493,  0.2659,\n",
       "            0.1790,  0.0869, -0.4193, -0.5062,  0.3579, -0.3835],\n",
       "          [ 0.0000, -0.1395,  0.3314,  0.2791,  0.2500, -0.4477, -0.0058,\n",
       "           -0.2209, -0.2267,  0.3372,  0.2558, -0.7442, -0.1570, -0.2326,\n",
       "           -0.3605,  0.0465,  0.0523,  0.4651,  0.2267,  0.1919],\n",
       "          [-0.2507, -0.0916,  0.2217,  0.1253,  0.0868,  0.3133, -0.0337,\n",
       "            0.3181, -0.0964, -0.1446,  0.5736, -0.0289,  0.3181, -0.3760,\n",
       "           -0.5302,  0.3953, -0.2314,  0.3230, -0.3230, -0.1928]]],\n",
       "        grad_fn=<CatBackward0>), scale=tensor(0.0057, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True)),\n",
       " QuantTensor(value=tensor([[[-0.0868,  0.6122,  0.1350,  0.0289, -0.1446,  0.1060,  0.1928,\n",
       "            0.3712, -0.3712, -0.1976, -0.0675,  0.4772, -0.1205, -0.3953,\n",
       "            0.0096, -0.2603,  0.2169,  0.1832,  0.3230,  0.0675],\n",
       "          [-0.2507, -0.0916,  0.2217,  0.1253,  0.0868,  0.3133, -0.0337,\n",
       "            0.3181, -0.0964, -0.1446,  0.5736, -0.0289,  0.3181, -0.3760,\n",
       "           -0.5302,  0.3953, -0.2314,  0.3230, -0.3230, -0.1928]]],\n",
       "        grad_fn=<UnsqueezeBackward0>), scale=tensor(0.0048, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True)))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantRNN\n",
    "\n",
    "quant_rnn = QuantRNN(input_size=10, hidden_size=20, batch_first=True, return_quant_tensor=True)\n",
    "quant_rnn(torch.randn(2, 5, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, a `QuantTensor` can be passed in as input. However, whenever `io_quant` is set (which it is by default), the input is gonna be re-quantized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(QuantTensor(value=tensor([[[-0.1020, -0.0089,  0.2661,  0.3016,  0.2040,  0.1419,  0.3815,\n",
       "           -0.1996,  0.1553,  0.0133,  0.2573,  0.1065, -0.3815,  0.1907,\n",
       "            0.1419,  0.0000,  0.5633, -0.2484, -0.3815,  0.2129],\n",
       "          [ 0.3667, -0.5927,  0.2964,  0.0201,  0.6379,  0.5827, -0.3817,\n",
       "           -0.2411,  0.1959, -0.3617, -0.1457,  0.0603,  0.5123, -0.3817,\n",
       "            0.2813,  0.0201, -0.5776, -0.0603,  0.5224, -0.0402],\n",
       "          [-0.3701,  0.0508,  0.3701,  0.2177,  0.2902,  0.4644, -0.0508,\n",
       "           -0.3338, -0.6821, -0.6676,  0.5152, -0.3991,  0.1524, -0.3265,\n",
       "           -0.2612,  0.1161,  0.0508,  0.5515,  0.2757, -0.2032],\n",
       "          [-0.0266,  0.0599,  0.3596,  0.2797,  0.5328, -0.3130,  0.3130,\n",
       "           -0.0466,  0.1265,  0.0000,  0.2264,  0.2731,  0.4529, -0.1865,\n",
       "           -0.2065,  0.3596, -0.0666, -0.1931,  0.2264,  0.0400],\n",
       "          [-0.3112,  0.0830,  0.0519,  0.2230,  0.0830, -0.1089,  0.1867,\n",
       "           -0.6069,  0.2801, -0.4201,  0.0259, -0.3320, -0.5602,  0.3008,\n",
       "           -0.1867,  0.6587,  0.5758, -0.4201, -0.3008,  0.2127]],\n",
       " \n",
       "         [[ 0.2129, -0.4613, -0.3149,  0.1464, -0.3504, -0.1863, -0.2795,\n",
       "           -0.1331, -0.1597, -0.0311, -0.4170,  0.0399,  0.4436, -0.2528,\n",
       "           -0.4436,  0.4835, -0.4170,  0.4037,  0.5633, -0.1153],\n",
       "          [ 0.4973, -0.2009, -0.4269,  0.2260, -0.5023, -0.4822,  0.3717,\n",
       "            0.5525, -0.0553,  0.4470,  0.2009, -0.4621,  0.4822, -0.0050,\n",
       "           -0.3064, -0.0352,  0.0301,  0.2311, -0.5475, -0.1457],\n",
       "          [ 0.1379,  0.4571, -0.8780,  0.2757, -0.7401, -0.9288,  0.8417,\n",
       "            0.4354,  0.7619,  0.0145, -0.1814,  0.6458, -0.3338,  0.6023,\n",
       "            0.0871, -0.4426,  0.5152, -0.7184, -0.6893,  0.4426],\n",
       "          [ 0.0200, -0.3263,  0.3929,  0.7792,  0.3929, -0.8525, -0.5261,\n",
       "            0.4729,  0.4729,  0.0400,  0.6194,  0.2997, -0.4862,  0.0200,\n",
       "           -0.0400,  0.5528,  0.0466,  0.4662, -0.4262,  0.6127],\n",
       "          [-0.0830,  0.4668, -0.3838,  0.4565,  0.4357,  0.3423, -0.4150,\n",
       "           -0.1400, -0.4772, -0.6172, -0.3942, -0.4201,  0.5239, -0.5135,\n",
       "           -0.1660,  0.1400, -0.0311, -0.2282,  0.1193, -0.2542]]],\n",
       "        grad_fn=<CatBackward0>), scale=tensor(0.0057, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True)),\n",
       " QuantTensor(value=tensor([[[-0.3112,  0.0830,  0.0519,  0.2230,  0.0830, -0.1089,  0.1867,\n",
       "           -0.6069,  0.2801, -0.4201,  0.0259, -0.3320, -0.5602,  0.3008,\n",
       "           -0.1867,  0.6587,  0.5758, -0.4201, -0.3008,  0.2127],\n",
       "          [-0.0830,  0.4668, -0.3838,  0.4565,  0.4357,  0.3423, -0.4150,\n",
       "           -0.1400, -0.4772, -0.6172, -0.3942, -0.4201,  0.5239, -0.5135,\n",
       "           -0.1660,  0.1400, -0.0311, -0.2282,  0.1193, -0.2542]]],\n",
       "        grad_fn=<UnsqueezeBackward0>), scale=tensor(0.0052, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True)))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "quant_identity = QuantIdentity(return_quant_tensor=True)\n",
    "quant_rnn(quant_identity(torch.randn(2, 5, 10)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with `torch.nn.RNN`, by default the initial hidden state is initialized to 0, but a custom hidden state of shape `(num_directions * num_layers, batch, hidden_size)` can be passed in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(QuantTensor(value=tensor([[[-0.5642, -0.4137,  0.5078, -0.7836,  0.5015, -0.2445, -0.7522,\n",
       "           -0.3322,  0.5391,  0.5767, -0.5516,  0.2633,  0.5892,  0.1442,\n",
       "           -0.4451,  0.5767, -0.2382, -0.8024, -0.5140, -0.2194],\n",
       "          [ 0.3816, -0.7834,  0.7298,  0.4151,  0.6963, -0.2009, -0.7834,\n",
       "            0.2477,  0.7499,  0.7030,  0.2209, -0.7566,  0.8101, -0.0201,\n",
       "           -0.5356,  0.8101, -0.6495,  0.5089, -0.3816, -0.7432],\n",
       "          [-0.4060,  0.5044, -0.0431,  0.2461,  0.1230, -0.4552,  0.3875,\n",
       "            0.4798, -0.1415, -0.1230,  0.1415, -0.5905,  0.0000, -0.4860,\n",
       "           -0.7074, -0.0615,  0.3506,  0.4060, -0.1845,  0.2830],\n",
       "          [-0.4431,  0.2933,  0.7488,  0.3869,  0.4119,  0.3994, -0.2247,\n",
       "            0.5367, -0.2184, -0.5928,  0.7676, -0.0312, -0.2808, -0.6740,\n",
       "           -0.5117,  0.2247, -0.0437,  0.7052, -0.3058,  0.3807],\n",
       "          [ 0.1529,  0.0453,  0.3680,  0.2152,  0.3001,  0.7191, -0.0226,\n",
       "           -0.2038, -0.3397, -0.3963, -0.1132,  0.4983,  0.6511, -0.3737,\n",
       "           -0.1132,  0.0963,  0.2265, -0.1868,  0.3794, -0.0453]],\n",
       " \n",
       "         [[ 0.2507, -0.1943,  0.0251,  0.1442, -0.2633, -0.5955,  0.0439,\n",
       "            0.1630,  0.6582,  0.6707,  0.7836,  0.7084, -0.0063,  0.3009,\n",
       "           -0.2884,  0.7898,  0.6269,  0.1943, -0.5642,  0.0690],\n",
       "          [-0.3749,  0.0937, -0.2076,  0.4821,  0.8503, -0.0201, -0.3013,\n",
       "           -0.1138,  0.1607, -0.6562,  0.1138, -0.5222,  0.0536, -0.4419,\n",
       "            0.1607,  0.1674, -0.0201,  0.0402, -0.5691,  0.0335],\n",
       "          [ 0.5352,  0.2953, -0.7136, -0.1661, -0.6951,  0.0308,  0.6951,\n",
       "            0.0000, -0.3383, -0.6705, -0.6705,  0.4983,  0.5659, -0.5475,\n",
       "            0.0308, -0.4983, -0.3445, -0.0861,  0.7812,  0.3260],\n",
       "          [-0.2808,  0.0998,  0.0998,  0.3994, -0.3120, -0.7988, -0.1373,\n",
       "            0.3307, -0.6428, -0.0437,  0.0499, -0.0686,  0.1685, -0.6740,\n",
       "            0.1997, -0.7988, -0.2371,  0.6490,  0.6428,  0.1685],\n",
       "          [ 0.3567, -0.6568, -0.2038, -0.2831, -0.5719, -0.4303,  0.0000,\n",
       "            0.4360, -0.0226, -0.5266, -0.0566,  0.1415,  0.3794, -0.7021,\n",
       "           -0.3794,  0.1246, -0.2718,  0.4416,  0.6568,  0.1642]]],\n",
       "        grad_fn=<CatBackward0>), scale=tensor(0.0062, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True)),\n",
       " QuantTensor(value=tensor([[[ 0.1529,  0.0453,  0.3680,  0.2152,  0.3001,  0.7191, -0.0226,\n",
       "           -0.2038, -0.3397, -0.3963, -0.1132,  0.4983,  0.6511, -0.3737,\n",
       "           -0.1132,  0.0963,  0.2265, -0.1868,  0.3794, -0.0453],\n",
       "          [ 0.3567, -0.6568, -0.2038, -0.2831, -0.5719, -0.4303,  0.0000,\n",
       "            0.4360, -0.0226, -0.5266, -0.0566,  0.1415,  0.3794, -0.7021,\n",
       "           -0.3794,  0.1246, -0.2718,  0.4416,  0.6568,  0.1642]]],\n",
       "        grad_fn=<UnsqueezeBackward0>), scale=tensor(0.0057, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_rnn(torch.randn(2, 5, 10), torch.randn(1, 2, 20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other Brevitas layers, `QuantRNN` can be initialized from a pretrained floating-point `torch.nn.RNN`. For the purpose of this tutorial, can simulate it from an untrained `torch.nn.RNN`. As for other quantized layers, setting `brevitas.config.IGNORE_MISSING_KEYS` might be necessary (depending on which quantizers are set). With the default quantizers, an error on activation scale keys would be triggered, so we set it to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import RNN\n",
    "from brevitas.nn import QuantRNN\n",
    "from brevitas import config\n",
    "\n",
    "config.IGNORE_MISSING_KEYS = True\n",
    "\n",
    "float_rnn = RNN(input_size=10, hidden_size=20)\n",
    "quant_rnn = QuantRNN(input_size=10, hidden_size=20)\n",
    "quant_rnn.load_state_dict(float_rnn.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to other quantized layers, quantization on a certain tensor can be disabled by setting a quantizer to `None`. Setting all quantizers to `None` recovers the same behaviour as the floating-point variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import RNN\n",
    "from brevitas.nn import QuantRNN\n",
    "from brevitas import config\n",
    "\n",
    "config.IGNORE_MISSING_KEYS = True\n",
    "torch.manual_seed(123456)\n",
    "\n",
    "float_rnn = RNN(input_size=10, hidden_size=20)\n",
    "quant_rnn = QuantRNN(input_size=10, hidden_size=20, weight_quant=None, io_quant=None, gate_acc_quant=None, bias_quant=None)\n",
    "\n",
    "# Set both layers to the same state_dict\n",
    "quant_rnn.load_state_dict(float_rnn.state_dict())\n",
    "\n",
    "# Generate random input\n",
    "inp = torch.randn(5, 2, 10)\n",
    "\n",
    "# Check outputs are the same\n",
    "assert torch.isclose(quant_rnn(inp)[0], float_rnn(inp)[0]).all().item()\n",
    "\n",
    "# Check hidden states are the same\n",
    "assert torch.isclose(quant_rnn(inp)[1], float_rnn(inp)[1]).all().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other quantized layers, we can leverage other prebuilt quantizers too. For example, to perform binary weight quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3684, -0.0946, -0.4480,  0.0050,  0.1543,  0.6322,  0.1643,\n",
       "            0.1693,  0.2937,  0.5227,  0.2290, -0.3534, -0.3883,  0.4331,\n",
       "            0.0000,  0.1693, -0.4331,  0.3634, -0.0050,  0.1941],\n",
       "          [-0.2240, -0.0199, -0.3534,  0.0946,  0.3485,  0.3534,  0.1941,\n",
       "            0.1643,  0.1145,  0.4082,  0.2987, -0.0647, -0.0946,  0.1543,\n",
       "            0.1145, -0.0498,  0.0647,  0.1493,  0.0299, -0.1195]],\n",
       " \n",
       "         [[ 0.0776, -0.0776, -0.5670,  0.4178, -0.0239,  0.4476,  0.2029,\n",
       "           -0.0836,  0.3521,  0.7042,  0.6326,  0.4058, -0.4118, -0.0477,\n",
       "           -0.2387, -0.0179, -0.4416, -0.4237, -0.3282, -0.1074],\n",
       "          [-0.2626,  0.3581,  0.2328, -0.2268, -0.2686, -0.3103,  0.4536,\n",
       "            0.3461,  0.3103,  0.3163,  0.3282, -0.3163, -0.7639,  0.0179,\n",
       "            0.0060,  0.0776, -0.5849, -0.5252,  0.1790,  0.2984]],\n",
       " \n",
       "         [[-0.5411,  0.3147,  0.6184, -0.3037, -0.1877, -0.3755,  0.1767,\n",
       "           -0.1767, -0.1491, -0.1049,  0.2871, -0.0552, -0.0883,  0.0331,\n",
       "            0.4749, -0.3147,  0.0331,  0.1767,  0.7013, -0.2264],\n",
       "          [-0.0773, -0.1877,  0.4749, -0.2264, -0.4583,  0.0166, -0.3534,\n",
       "           -0.5743,  0.5411,  0.1160, -0.0442, -0.0442,  0.3037,  0.0166,\n",
       "           -0.1325, -0.1657, -0.0718,  0.1215,  0.6240,  0.3092]],\n",
       " \n",
       "         [[-0.0627, -0.1882, -0.4642, -0.1443,  0.4705,  0.3137, -0.2447,\n",
       "            0.0063, -0.1129,  0.3011,  0.1882,  0.2572,  0.2384, -0.0376,\n",
       "            0.1129, -0.1380,  0.1380,  0.3011, -0.0251, -0.0063],\n",
       "          [-0.6399,  0.5771,  0.2133,  0.2572,  0.7967,  0.1631, -0.2384,\n",
       "           -0.4078, -0.3199,  0.0753,  0.6524,  0.0690, -0.1819, -0.2258,\n",
       "            0.3889, -0.4078, -0.3764,  0.2258,  0.5458, -0.1756]],\n",
       " \n",
       "         [[-0.5704,  0.6139, -0.1209, -0.5173,  0.4447,  0.0048,  0.3481,\n",
       "           -0.5946, -0.5221,  0.1644, -0.2949, -0.1789, -0.1982,  0.2707,\n",
       "            0.2900, -0.5124, -0.4399, -0.0725,  0.4351,  0.6091],\n",
       "          [ 0.0435,  0.2030, -0.4447, -0.2659,  0.1547,  0.0580,  0.4254,\n",
       "            0.5559,  0.1740,  0.4254,  0.4592,  0.2369, -0.4496, -0.3336,\n",
       "            0.3046,  0.1354, -0.3626, -0.2659, -0.2079, -0.4641]]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " tensor([[[-0.5704,  0.6139, -0.1209, -0.5173,  0.4447,  0.0048,  0.3481,\n",
       "           -0.5946, -0.5221,  0.1644, -0.2949, -0.1789, -0.1982,  0.2707,\n",
       "            0.2900, -0.5124, -0.4399, -0.0725,  0.4351,  0.6091],\n",
       "          [ 0.0435,  0.2030, -0.4447, -0.2659,  0.1547,  0.0580,  0.4254,\n",
       "            0.5559,  0.1740,  0.4254,  0.4592,  0.2369, -0.4496, -0.3336,\n",
       "            0.3046,  0.1354, -0.3626, -0.2659, -0.2079, -0.4641]]],\n",
       "        grad_fn=<UnsqueezeBackward0>))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.quant.binary import SignedBinaryWeightPerTensorConst\n",
    "\n",
    "binary_rnn = QuantRNN(input_size=10, hidden_size=20, weight_quant=SignedBinaryWeightPerTensorConst)\n",
    "binary_rnn(torch.randn(5, 2, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuantLSTM\n",
    "\n",
    "We now look at `QuantLSTM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "    def __init__(\n",
       "            self,\n",
       "            input_size: int,\n",
       "            hidden_size: int,\n",
       "            num_layers: int = 1,\n",
       "            bias: bool = True,\n",
       "            batch_first: bool = False,\n",
       "            bidirectional: bool = False,\n",
       "            weight_quant = Int8WeightPerTensorFloat,\n",
       "            bias_quant = Int32Bias,\n",
       "            io_quant = Int8ActPerTensorFloat,\n",
       "            gate_acc_quant = Int8ActPerTensorFloat,\n",
       "            sigmoid_quant = Uint8ActPerTensorFloat,\n",
       "            tanh_quant = Int8ActPerTensorFloat,\n",
       "            cell_state_quant = Int8ActPerTensorFloat,\n",
       "            coupled_input_forget_gates: bool = False,\n",
       "            cat_output_cell_states = True,\n",
       "            shared_input_hidden_weights = False,\n",
       "            shared_intra_layer_weight_quant = False,\n",
       "            shared_intra_layer_gate_acc_quant = False,\n",
       "            shared_cell_state_quant = True,\n",
       "            return_quant_tensor: bool = False,\n",
       "            **kwargs):\n",
       "        super(QuantLSTM, self).__init__(\n",
       "            layer_impl=_QuantLSTMLayer,\n",
       "            input_size=input_size,\n",
       "            hidden_size=hidden_size,\n",
       "            num_layers=num_layers,\n",
       "            bias=bias,\n",
       "            batch_first=batch_first,\n",
       "            bidirectional=bidirectional,\n",
       "            weight_quant=weight_quant,\n",
       "            bias_quant=bias_quant,\n",
       "            io_quant=io_quant,\n",
       "            gate_acc_quant=gate_acc_quant,\n",
       "            sigmoid_quant=sigmoid_quant,\n",
       "            tanh_quant=tanh_quant,\n",
       "            cell_state_quant=cell_state_quant,\n",
       "            cifg=coupled_input_forget_gates,\n",
       "            shared_input_hidden_weights=shared_input_hidden_weights,\n",
       "            shared_intra_layer_weight_quant=shared_intra_layer_weight_quant,\n",
       "            shared_intra_layer_gate_acc_quant=shared_intra_layer_gate_acc_quant,\n",
       "            shared_cell_state_quant=shared_cell_state_quant,\n",
       "            return_quant_tensor=return_quant_tensor,\n",
       "            **kwargs)\n",
       "        if cat_output_cell_states and cell_state_quant is not None and not shared_cell_state_quant:\n",
       "            raise RuntimeError(\"Concatenating cell states requires shared cell quantizers.\")\n",
       "        self.cat_output_cell_states = cat_output_cell_states\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import inspect\n",
    "from brevitas.nn import QuantLSTM\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def pretty_print_source(source):\n",
    "    display(Markdown('```python\\n' + source + '\\n```'))\n",
    "    \n",
    "source = inspect.getsource(QuantLSTM.__init__)  \n",
    "pretty_print_source(source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with `QuantRNN`, `QuantLSTM` supports all options of `torch.nn.LSTM`. Everything said so far on `QuantRNN` applies to `QuantLSTM` too, but there a bunch of things more to be aware of.\n",
    "\n",
    "`QuantLSTM` accepts a few more quantizers: `sigmoid_quant`, `tanh_quant` and `cell_state_quant`. As with `QuantRNN`, setting `bidirectional=True` and/or `num_layers > 1` triggers sharing the instance of certain quantizers, but not others. In particular `io_quant` is shared among all layers and directions, as it was the case for `QuantRNN`. `cell_state_quant` is shared by default, but setting `shared_cell_state_quant=False` can disable that. However, that requires setting `cat_output_cell_states=False`, as otherwise we would find ourselves with a concenation of cell states that have been quantized with different quantizers, which is considered illegal in Brevitas.\n",
    "\n",
    "LSTMs have four gates, each with its input-hidden and hidden-hidden weights. Brevitas takes in one `weight_quant` definition, but then four different instances of the weight quantizer are instantiated, and each gate is quantized differently, meaning it can have its own scale and zero-point. To force sharing the same weight quantizer across all gates, `QuantLSTM` supports setting `shared_intra_layer_weight_quant=True`. The same reasoning applies to the quantization of the output of each gate, before the activation functions, which is controlled by the `gate_acc_quant` quantizer. To force the same quantizer instance to be shared, `shared_intra_layer_gate_acc_quant=True` can be set. Different sigmoid and tanh functions instead are always allocated different quantizer instances.\n",
    "\n",
    "Finally, `QuantLSTM` also supports the coupled input-forget gates (CIFG), where the forget gate is defined as `forget_gate = 1 - input_gate`, by setting `coupled_input_forget_gates=True`. This is an optimization to save on some compute and number of parameters, and is orthogonal to all other settings, such as `shared_input_hidden_weights`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just-in-time compilation\n",
    "\n",
    "Custom recurrent layer can be quite slow at training time. With quantization added in, it only gets worse. To mitigate the issue, both `QuantRNN` and `QuantLSTM` support jit compilation. Setting the env variable `BREVITAS_JIT=1` triggers end-to-end compilation of the quantized recurrent cell through PyTorch TorchScript compiler."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "As of version 0.8 of Brevitas, `QuantRNN` and `QuantLSTM` don't support quantized activations calibration through `calibration_mode `nor bias correction through `bias_correction_mode`. This will be added in a future version."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "As of Brevitas 0.8, export of quantized recurrent layers is still a work in progress. As a proof of concept, there is support only for export of `QuantLSTM` to QONNX, a custom set of quantized operators introduced by Brevitas on top of ONNX. Two use cases are supported: (1) only `weight_quant` is set, and (2) all quantizers are set. In both cases, `bidirectional=True` and `num_layers > 1` are supported. We first define an utility function to visualize the network through netron, which requires `pip install netron`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def show_netron(model_path, port):\n",
    "    try:\n",
    "        import netron\n",
    "        time.sleep(3.)\n",
    "        netron.start(model_path, address=(\"localhost\", port), browse=False)\n",
    "        return IFrame(src=f\"http://localhost:{port}/\", width=\"100%\", height=400)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuantLSTM weight-only quantization export\n",
    "\n",
    "For use case (1), weight quantization is represented with `Quant` nodes, while the standard ONNX `LSTM` operator is adopted for the recurrent cell. Opset 14 is required:. For the 1 layer, 1 direction use case we keep the default `weight_quant` set, while we disable the other quantizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantLSTM\n",
    "from brevitas.export import export_qonnx\n",
    "\n",
    "quant_lstm_weight_only = QuantLSTM(input_size=10, hidden_size=20, io_quant=None, bias_quant=None, gate_acc_quant=None, sigmoid_quant=None, tanh_quant=None, cell_state_quant=None)\n",
    "export_path = 'quant_lstm_weight_only.onnx'\n",
    "export_qonnx(quant_lstm_weight_only, (torch.randn(5, 2, 10)), opset_version=14, export_path=export_path)\n",
    "show_netron(export_path, 8080)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFG is also supported in a way that follows the semantics of ONNXRuntime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantLSTM\n",
    "from brevitas.export import export_qonnx\n",
    "\n",
    "quant_lstm_weight_only_cifg = QuantLSTM(\n",
    "    input_size=10, hidden_size=20, coupled_input_forget_gates=True, \n",
    "    io_quant=None, bias_quant=None, gate_acc_quant=None, sigmoid_quant=None, tanh_quant=None, cell_state_quant=None)\n",
    "export_path = 'quant_lstm_weight_only_cifg.onnx'\n",
    "export_qonnx(quant_lstm_weight_only_cifg, (torch.randn(5, 2, 10)), opset_version=14, export_path=export_path)\n",
    "show_netron(export_path, 8082)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 2 layers, 2 directions use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantLSTM\n",
    "from brevitas.export import export_qonnx\n",
    "\n",
    "quant_lstm_weight_only_bidirectional_2_layers = QuantLSTM(\n",
    "    input_size=10, hidden_size=20, bidirectional=True, num_layers=2, \n",
    "    io_quant=None, bias_quant=None, gate_acc_quant=None, sigmoid_quant=None, tanh_quant=None, cell_state_quant=None)\n",
    "export_path = 'quant_lstm_weight_only_bidirectional_2_layers.onnx'\n",
    "export_qonnx(quant_lstm_weight_only_bidirectional_2_layers, (torch.randn(5, 2, 10)), opset_version=14, export_path=export_path)\n",
    "show_netron(export_path, 8083)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared input-hidden weights are also supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantLSTM\n",
    "from brevitas.export import export_qonnx\n",
    "\n",
    "quant_lstm_weight_only_bidirectional_2_layers_shared = QuantLSTM(\n",
    "    input_size=10, hidden_size=20, bidirectional=True, shared_input_hidden_weights=True,\n",
    "    io_quant=None, bias_quant=None, gate_acc_quant=None, sigmoid_quant=None, tanh_quant=None, cell_state_quant=None)\n",
    "export_path = 'quant_lstm_weight_only_bidirectional_2_layers_shared.onnx'\n",
    "export_qonnx(quant_lstm_weight_only_bidirectional_2_layers_shared, (torch.randn(5, 2, 10)), opset_version=14, export_path=export_path)\n",
    "show_netron(export_path, 8085)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuantLSTM full quantization export\n",
    "\n",
    "For use case (3), weight quantization is represented with `Quant` nodes, while a custom quantized LSTM operator `QuantLSTMCell` operator is generated for the recurrent cell. In a future version of Brevitas, `QuantLSTMCell` will instead be lowered to a series of standard ops + `Quant` nodes. For the purpose example, we keep all quantizers at default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantLSTM\n",
    "from brevitas.export import export_qonnx\n",
    "\n",
    "quant_lstm = QuantLSTM(input_size=10, hidden_size=20)\n",
    "export_path = 'quant_lstm.onnx'\n",
    "export_qonnx(quant_lstm, (torch.randn(5, 2, 10)), export_path=export_path)\n",
    "show_netron(export_path, 8086)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QuantLSTMCell` takes the following series of inputs:\n",
    "\n",
    "- quant_input, \n",
    "- quant_hidden_state, \n",
    "- quant_cell_state,\n",
    "- quant_weight_ii, \n",
    "- quant_weight_if,\n",
    "- quant_weight_ic,\n",
    "- quant_weight_io,\n",
    "- quant_weight_hi, \n",
    "- quant_weight_hf,\n",
    "- quant_weight_hc,\n",
    "- quant_weight_ho,\n",
    "- quant_bias_input,\n",
    "- quant_bias_forget,\n",
    "- quant_bias_cell,\n",
    "- quant_bias_output,\n",
    "- output_scale, \n",
    "- output_zero_point, \n",
    "- output_bit_width, \n",
    "- cell_state_scale, \n",
    "- cell_state_zero_point, \n",
    "- cell_state_bit_width, \n",
    "- input_acc_scale, \n",
    "- input_acc_zero_point,\n",
    "- input_acc_bit_width, \n",
    "- forget_acc_scale, \n",
    "- forget_acc_zero_point, \n",
    "- forget_acc_bit_width, \n",
    "- cell_acc_scale, \n",
    "- cell_acc_zero_point, \n",
    "- cell_acc_bit_width, \n",
    "- output_acc_scale, \n",
    "- output_acc_zero_point, \n",
    "- output_acc_bit_width, \n",
    "- input_sigmoid_scale, \n",
    "- input_sigmoid_zero_point, \n",
    "- input_sigmoid_bit_width, \n",
    "- forget_sigmoid_scale, \n",
    "- forget_sigmoid_zero_point, \n",
    "- forget_sigmoid_bit_width, \n",
    "- cell_tanh_scale, \n",
    "- cell_tanh_zero_point, \n",
    "- cell_tanh_bit_width, \n",
    "- output_sigmoid_scale, \n",
    "- output_sigmoid_zero_point, \n",
    "- output_sigmoid_bit_width, \n",
    "- hidden_state_tanh_scale, \n",
    "- hidden_state_tanh_zero_point, \n",
    "- hidden_state_tanh_bit_width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All previous use cases illustrated for the weight-only quantization scenario are also supported."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_latest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6e150ee02c45d2c3f896173a651a21b25567e05411969bcc0f3a62fa15a0a0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
