{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# An overview of QuantTensor and QuantConv2d\n",
    "\n",
    "In this initial tutorial, we take a first look at `QuantTensor`, a basic data structure in Brevitas, and at `QuantConv2d`, a typical quantized layer. `QuantConv2d` is an instance of a `QuantWeightBiasInputOutputLayer` (typically imported as `QuantWBIOL`), meaning that it supports quantization of its weight, bias, input and output. Other instances of `QuantWBIOL` are `QuantLinear`, `QuantConv1d`, `QuantConvTranspose1d` and `QuantConvTranspose2d`, and they all follow the same principles.\n",
    "\n",
    "If we take a look at the `__init__` method of `QuantConv2d`, we notice a few things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "    def __init__(\n",
       "            self,\n",
       "            in_channels: int,\n",
       "            out_channels: int,\n",
       "            kernel_size: Union[int, Tuple[int, int]],\n",
       "            stride: Union[int, Tuple[int, int]] = 1,\n",
       "            padding: Union[int, Tuple[int, int]] = 0,\n",
       "            dilation: Union[int, Tuple[int, int]] = 1,\n",
       "            groups: int = 1,\n",
       "            padding_mode: str = 'zeros',\n",
       "            bias: bool = True,\n",
       "            weight_quant: Optional[WeightQuantType] = Int8WeightPerTensorFloat,\n",
       "            bias_quant: Optional[BiasQuantType] = None,\n",
       "            input_quant: Optional[ActQuantType] = None,\n",
       "            output_quant: Optional[ActQuantType] = None,\n",
       "            return_quant_tensor: bool = False,\n",
       "            device: Optional[torch.device] = None,\n",
       "            dtype: Optional[torch.dtype] = None,\n",
       "            **kwargs) -> None:\n",
       "        # avoid an init error in the super class by setting padding to 0\n",
       "        if padding_mode == 'zeros' and padding == 'same' and stride > 1:\n",
       "            padding = 0\n",
       "            is_same_padded_strided = True\n",
       "        else:\n",
       "            is_same_padded_strided = False\n",
       "        Conv2d.__init__(\n",
       "            self,\n",
       "            in_channels=in_channels,\n",
       "            out_channels=out_channels,\n",
       "            kernel_size=kernel_size,\n",
       "            stride=stride,\n",
       "            padding=padding,\n",
       "            padding_mode=padding_mode,\n",
       "            dilation=dilation,\n",
       "            groups=groups,\n",
       "            bias=bias,\n",
       "            device=device,\n",
       "            dtype=dtype)\n",
       "        QuantWBIOL.__init__(\n",
       "            self,\n",
       "            weight_quant=weight_quant,\n",
       "            bias_quant=bias_quant,\n",
       "            input_quant=input_quant,\n",
       "            output_quant=output_quant,\n",
       "            return_quant_tensor=return_quant_tensor,\n",
       "            **kwargs)\n",
       "        self.is_same_padded_strided = is_same_padded_strided\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import inspect\n",
    "from brevitas.nn import QuantConv2d\n",
    "from brevitas.nn import QuantIdentity\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def pretty_print_source(source):\n",
    "    display(Markdown('```python\\n' + source + '\\n```'))\n",
    "    \n",
    "source = inspect.getsource(QuantConv2d.__init__)  \n",
    "pretty_print_source(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QuantConv2d` is an instance of both `Conv2d` and `QuantWBIOL`. Its initialization method exposes the usual arguments of a `Conv2d`, as well as: an extra flag to support *same padding*; *four* different arguments to set a quantizer for - respectively - *weight*, *bias*, *input*, and *output*; a `return_quant_tensor` boolean flag; the `**kwargs` placeholder to intercept additional arbitrary keyword arguments.  \n",
    "In this tutorial we will focus on how to set the four quantizer arguments and the return flags; arbitrary kwargs will be explained in a separate tutorial dedicated to defining and overriding quantizers.\n",
    "\n",
    "By default `weight_quant=Int8WeightPerTensorFloat`, while `bias_quant`, `input_quant` and `output_quant` are set to `None`. That means that by default weights are quantized to *8-bit signed integer with a per-tensor floating-point scale factor* (a very common type of quantization adopted by e.g. the ONNX standard opset), while quantization of bias, input, and output are disabled. We can easily verify all of this at runtime on an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is weight quant enabled: True\n",
      "Is bias quant enabled: False\n",
      "Is input quant enabled: False\n",
      "Is output quant enabled: False\n"
     ]
    }
   ],
   "source": [
    "print(f'Is weight quant enabled: {default_quant_conv.is_weight_quant_enabled}')\n",
    "print(f'Is bias quant enabled: {default_quant_conv.is_bias_quant_enabled}')\n",
    "print(f'Is input quant enabled: {default_quant_conv.is_input_quant_enabled}')\n",
    "print(f'Is output quant enabled: {default_quant_conv.is_output_quant_enabled}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now try to pass in a random floating-point tensor as input, as expected we get the output of the convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giuseppe/miniconda3/envs/torch_2.1/lib/python3.11/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0093,  0.4820,  0.0156],\n",
       "          [-0.1535, -0.2748, -0.9393],\n",
       "          [-1.0662,  0.2397,  0.0932]],\n",
       "\n",
       "         [[ 0.6932, -0.2772,  0.0703],\n",
       "          [ 0.2536,  0.1734, -0.3745],\n",
       "          [-0.5633,  0.2231, -0.6844]],\n",
       "\n",
       "         [[-0.2607,  0.2174, -0.0522],\n",
       "          [ 0.1215, -0.3744, -0.5880],\n",
       "          [-0.3104, -0.6930,  0.5322]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "out = default_quant_conv(torch.randn(1, 2, 5, 5))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are computing the convolution between an unquantized input tensor and quantized weights, so the output in general is unquantized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A QuantConv2d with quantization disabled everywhere behaves like a standard `Conv2d`. Again can easily verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "\n",
    "torch.manual_seed(0)  # set a seed to make sure the random weight init is reproducible\n",
    "disabled_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, weight_quant=None)\n",
    "torch.manual_seed(0)  # reproduce the same random weight init as above\n",
    "float_conv = Conv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False)\n",
    "inp = torch.randn(1, 2, 5, 5)\n",
    "assert torch.isclose(disabled_quant_conv(inp), float_conv(inp)).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have just seen, Brevitas allows users as much freedom as possible to experiment with quantization, meaning that computation between quantized and unquantized values is considered legal. This allows users to mix Brevitas layers with Pytorch layers with little restrictions.  \n",
    "To make this possible, quantized values are typically represented in *dequantized format*, meaning that - in the case of affine quantization implemented in Brevitas - zero-point and scale factor are applied to their integer values according to the formula **quant_value = (integer_value - zero_point) * scale**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuantTensor\n",
    "\n",
    "We can directly observe the quantized weights by calling the weight quantizer on the layer's weights: `default_quant_conv.weight_quant(quant_conv.weight)`, which for shortness is already implemented as `default_quant_conv.quant_weight()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[ 0.0236,  0.1599,  0.1799],\n",
       "          [-0.0545,  0.2144,  0.2126],\n",
       "          [-0.1363, -0.2271, -0.1526]],\n",
       "\n",
       "         [[-0.0872, -0.0091, -0.1090],\n",
       "          [ 0.0690, -0.0327,  0.2289],\n",
       "          [ 0.2307,  0.0073, -0.1326]]],\n",
       "\n",
       "\n",
       "        [[[-0.0254,  0.0418, -0.0363],\n",
       "          [-0.2053,  0.2071, -0.1163],\n",
       "          [-0.1163, -0.1653,  0.0109]],\n",
       "\n",
       "         [[-0.2107, -0.1199,  0.0799],\n",
       "          [ 0.0200,  0.0218,  0.1817],\n",
       "          [-0.1199, -0.0963, -0.0600]]],\n",
       "\n",
       "\n",
       "        [[[-0.0709, -0.0908,  0.1544],\n",
       "          [-0.0236, -0.2235,  0.2180],\n",
       "          [-0.0799, -0.0200,  0.0273]],\n",
       "\n",
       "         [[-0.1998,  0.1126,  0.1435],\n",
       "          [ 0.0818,  0.1399,  0.1181],\n",
       "          [ 0.1762, -0.1726, -0.2216]]]], grad_fn=<MulBackward0>), scale=tensor(0.0018, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the quantized weights are wrapped in a data structure implemented by Brevitas called `QuantTensor`. A `QuantTensor` is a way to represent an affine quantized tensor with all its metadata, meaning: the `value` of the quantized tensor in *dequantized* format, `scale`, `zero_point`, `bit_width`, whether the quantized value it's `signed` or not, and whether the tensor was generated in `training` mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have that the quantized value (in dequantized format) can be computer from its integer representation, together with zero-point and scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_weight = default_quant_conv.int_weight()\n",
    "zero_point = default_quant_conv.quant_weight_zero_point()\n",
    "scale = default_quant_conv.quant_weight_scale()\n",
    "quant_weight_manually = (int_weight - zero_point) * scale\n",
    "\n",
    "assert default_quant_conv.quant_weight().value.isclose(quant_weight_manually).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *valid* QuantTensor correctly populates all its fields with values `!= None` and respect the **affine quantization invariant**, i.e. `value / scale + zero_point` is (accounting for rounding errors) an *integer* that can be represented within the interval defined by the `bit_width` and `signed` fields of the `QuantTensor`. A *non-valid* one doesn't.\n",
    "We can observe that the quantized weights are indeed marked as valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert default_quant_conv.quant_weight().is_valid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `is_valid` is relative expensive, so it should be using sparingly, but there are a few cases where a non-valid QuantTensor might be generated that is important to be aware of. Say we have two QuantTensor as output of the same quantized activation, and we want to sum them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0211, grad_fn=<DivBackward0>)\n",
      "tensor(0.0162, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from brevitas.quant_tensor import QuantTensor\n",
    "\n",
    "quant_act = QuantIdentity(return_quant_tensor=True)\n",
    "\n",
    "out_tensor_0 = quant_act(torch.randn(1,2,5,5))\n",
    "out_tensor_1 = quant_act(torch.randn(1,2,5,5))\n",
    "\n",
    "assert out_tensor_0.is_valid\n",
    "assert out_tensor_1.is_valid\n",
    "print(out_tensor_0.scale)\n",
    "print(out_tensor_1.scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both QuantTensor are valid but since the quantized activation is in training mode by default, their scale factors are going to be different. It is important to note that the behaviour is different at evaluation time, where the two scale factors will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantTensor(value=tensor([[[[-0.1106,  1.1945, -0.4972, -2.0968,  0.7175],\n",
      "          [-2.5901,  0.0588, -0.2014,  2.1486,  1.6435],\n",
      "          [ 0.9067, -2.5212,  2.2193,  0.2352, -0.8395],\n",
      "          [-0.8351,  0.6341, -0.5551,  0.1040, -3.3151],\n",
      "          [-0.8979, -0.7092,  3.8232,  1.0875,  0.3954]],\n",
      "\n",
      "         [[ 1.4363, -1.3973,  1.3249,  2.6914,  0.3660],\n",
      "          [ 1.5057,  1.8094,  0.5100, -1.6874,  1.9981],\n",
      "          [ 1.2472, -1.7813,  0.0334, -1.2880, -2.9333],\n",
      "          [ 0.0180, -1.4298, -2.9978,  0.5494, -1.4548],\n",
      "          [ 1.6738, -0.3177, -0.3721, -0.1650, -1.1871]]]],\n",
      "       grad_fn=<AddBackward0>), scale=0.018651068210601807, zero_point=0.0, bit_width=9.0, signed_t=True, training_t=True)\n"
     ]
    }
   ],
   "source": [
    "out_tensor = out_tensor_0 + out_tensor_1\n",
    "print(out_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set `training` to `True` for both of them, we are allowed to sum them even if they have different scale factors. The output QuantTensor will have the correct `bit_width`, and a scale which is the average of the two original scale factors. This is done only at training time, in order to propagate gradient information, however the consequence is that the resulting QuantTensor is no longer valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not out_tensor.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QuantTensor` implements `__torch_function__` to handle being called from torch functional operators (e.g. ops under `torch.nn.functional`). Passing a QuantTensor to supported ops that are invariant to quantization, e.g. max-pooling, preserve the the validity of a QuantTensor. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[0.5191, 0.6402],\n",
       "          [2.1455, 0.5883]],\n",
       "\n",
       "         [[2.0417, 0.5883],\n",
       "          [1.2631, 0.3980]],\n",
       "\n",
       "         [[0.7959, 0.5191],\n",
       "          [0.8132, 1.3496]]]], grad_fn=<MaxPool2DWithIndicesBackward0>), scale=tensor(0.0173, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "quant_identity = QuantIdentity(return_quant_tensor=True)\n",
    "quant_tensor = quant_identity(torch.randn(1, 3, 4, 4))\n",
    "torch.nn.functional.max_pool2d(quant_tensor, kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ops that are not invariant to quantization, a `QuantTensor` decays into a floating-point `torch.Tensor`. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2482988/1377665000.py:1: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/torch/csrc/utils/python_arg_parser.cpp:368.)\n",
      "  torch.tanh(quant_tensor)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4770,  0.2212,  0.0691,  0.5650],\n",
       "          [-0.0346, -0.6618, -0.4635, -0.3482],\n",
       "          [ 0.9730, -0.7245, -0.5881, -0.5287],\n",
       "          [-0.0863,  0.8857,  0.5287, -0.4498]],\n",
       "\n",
       "         [[ 0.9669,  0.5650, -0.6211, -0.4498],\n",
       "          [-0.2376,  0.6103,  0.5287,  0.2700],\n",
       "          [-0.6808,  0.8519,  0.2700, -0.5531],\n",
       "          [-0.0173,  0.8264,  0.3782, -0.1881]],\n",
       "\n",
       "         [[-0.6211, -0.9764, -0.5993,  0.4770],\n",
       "          [ 0.5033,  0.6618, -0.1881, -0.6211],\n",
       "          [-0.8031,  0.1375,  0.5287,  0.8740],\n",
       "          [-0.6714,  0.6714, -0.5650,  0.8611]]]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(quant_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Quantization\n",
    "\n",
    "We can obtain a valid output `QuantTensor` by making sure that both input and weight of `QuantConv2d` are quantized. To do so, we can set a quantizer for `input_quant`. In this example we pick a *signed 8-bit* quantizer with *per-tensor floating-point scale factor*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.3568, -0.1883,  0.3589],\n",
       "          [-0.4470,  0.1039, -0.3945],\n",
       "          [-0.4190,  0.3723,  0.8384]],\n",
       "\n",
       "         [[-0.0510,  0.5514, -0.2751],\n",
       "          [-0.5668,  0.5824,  0.2328],\n",
       "          [ 0.1316, -0.2518,  1.0418]],\n",
       "\n",
       "         [[ 0.2734,  0.7268, -0.0249],\n",
       "          [-0.1732,  0.5197,  1.1158],\n",
       "          [ 0.3771, -0.3810,  0.2008]]]], grad_fn=<ConvolutionBackward0>), scale=tensor([[[[3.1958e-05]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
    "\n",
    "input_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, \n",
    "    input_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
    "out_tensor = input_quant_conv(torch.randn(1, 2, 5, 5))\n",
    "out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out_tensor.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens internally is that the input tensor passed to `input_quant_conv` is being quantized before being passed to the convolution operator. That means we are now computing a convolution between two quantized tensors, which mimplies that the output of the operation is also quantized. As expected then `out_tensor` is marked as valid. \n",
    "\n",
    "Another important thing to notice is how the `bit_width` field of `out_tensor` is relatively high at *21 bits*. In Brevitas, the assumption is always that the output bit-width of an operator reflects the worst-case size of the *accumulator* required by that operation. In other terms, given the *size* of the input and weight tensors and their *bit-widths*, 21 is the bit-width that would be required to represent the largest possible output value that could be generated. This makes sure that the affine quantization invariant is always respected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have obtained a similar result by directly passing as input a QuantTensor. In this example we are directly defining a QuantTensor ourselves, but it could also be the output of a previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[ 7.2000e-03, -3.7000e-03,  7.7000e-03, -2.4000e-03, -8.9000e-03],\n",
       "          [-1.2000e-02, -8.1000e-03,  7.2000e-03, -1.1300e-02, -9.7000e-03],\n",
       "          [-1.0000e-03,  1.0100e-02,  3.8000e-03, -1.1900e-02,  6.9000e-03],\n",
       "          [ 8.3000e-03,  1.0000e-04, -6.9000e-03,  3.9000e-03, -5.4000e-03],\n",
       "          [ 1.1300e-02, -6.0000e-03,  9.7000e-03,  0.0000e+00,  1.0900e-02]],\n",
       "\n",
       "         [[-1.0900e-02,  1.1400e-02, -6.4000e-03,  9.2000e-03,  7.1000e-03],\n",
       "          [-6.0000e-04,  9.2000e-03, -8.5000e-03,  5.0000e-03,  6.5000e-03],\n",
       "          [-8.3000e-03, -1.2000e-03,  7.4000e-03,  9.2000e-03, -6.0000e-04],\n",
       "          [-2.1000e-03,  9.5000e-03,  3.0000e-04, -2.9000e-03, -6.5000e-03],\n",
       "          [-1.1800e-02, -4.8000e-03,  5.4000e-03, -2.5000e-03,  9.0000e-04]]]]), scale=tensor(1.0000e-04), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.quant_tensor import QuantTensor\n",
    "\n",
    "scale = 0.0001\n",
    "bit_width = 8\n",
    "zero_point = 0.\n",
    "int_value = torch.randint(low=- 2 ** (bit_width - 1), high=2 ** (bit_width - 1) - 1, size=(1, 2, 5, 5))\n",
    "quant_value = (int_value - zero_point) * scale\n",
    "quant_tensor_input = QuantTensor(\n",
    "    quant_value, \n",
    "    scale=torch.tensor(scale), \n",
    "    zero_point=torch.tensor(zero_point), \n",
    "    bit_width=torch.tensor(float(bit_width)),\n",
    "    signed=True,\n",
    "    training=True)\n",
    "quant_tensor_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert quant_tensor_input.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: how we are explicitly forcing `value`, `scale`, `zero_point` and `bit_width` to be floating-point `torch.Tensor`, as this is expected by Brevitas but it's currently not enforced automatically at initialization time.\n",
    "\n",
    "If we now pass in `quant_tensor_input` to `return_quant_conv`, we will see that indeed the output is a valid 21-bit `QuantTensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.0019,  0.0049, -0.0012],\n",
       "          [-0.0012,  0.0050, -0.0074],\n",
       "          [-0.0023, -0.0035, -0.0033]],\n",
       "\n",
       "         [[-0.0031,  0.0028,  0.0116],\n",
       "          [ 0.0079,  0.0046,  0.0022],\n",
       "          [ 0.0021, -0.0004,  0.0011]],\n",
       "\n",
       "         [[-0.0045, -0.0010,  0.0002],\n",
       "          [-0.0044,  0.0027,  0.0025],\n",
       "          [-0.0009,  0.0040, -0.0044]]]], grad_fn=<ConvolutionBackward0>), scale=tensor([[[[1.8307e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, return_quant_tensor=True)\n",
    "out_tensor = return_quant_conv(quant_tensor_input)\n",
    "out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out_tensor.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass in an input `QuantTensor` to a layer that has `input_quant` enabled. In that case, the input gets re-quantized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.0073,  0.0040, -0.0011],\n",
       "          [-0.0033,  0.0078, -0.0028],\n",
       "          [ 0.0005, -0.0025, -0.0008]],\n",
       "\n",
       "         [[ 0.0021, -0.0021,  0.0035],\n",
       "          [ 0.0012, -0.0016, -0.0023],\n",
       "          [-0.0010, -0.0015,  0.0040]],\n",
       "\n",
       "         [[-0.0010,  0.0047,  0.0025],\n",
       "          [-0.0014,  0.0021, -0.0039],\n",
       "          [ 0.0036, -0.0003,  0.0026]]]], grad_fn=<ConvolutionBackward0>), scale=tensor([[[[1.7393e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_quant_conv(quant_tensor_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Quantization\n",
    "\n",
    "Let's now look at would have happened if we instead enabled output quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.2117, -0.4811,  0.0385],\n",
       "          [-0.5100, -0.2502, -0.2213],\n",
       "          [-0.5773,  0.0192, -0.5485]],\n",
       "\n",
       "         [[ 0.1347,  0.8179, -1.2316],\n",
       "          [-0.6062,  0.4426, -0.3849],\n",
       "          [ 0.1732, -0.5100, -0.1251]],\n",
       "\n",
       "         [[ 1.0873,  0.2406, -0.2887],\n",
       "          [-0.4330, -0.4907, -0.2021],\n",
       "          [ 0.6447,  0.4811,  0.1347]]]], grad_fn=<MulBackward0>), scale=tensor(0.0096, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
    "\n",
    "output_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, \n",
    "    output_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
    "out_tensor = output_quant_conv(torch.randn(1, 2, 5, 5))\n",
    "out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out_tensor.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see again that the output is a valid `QuantTensor`. However, what happened internally is quite different from before.  \n",
    "Previously, we computed the convolution between two quantized tensors, and got a quantized tensor as output.  \n",
    "In this case instead, we compute the convolution between a quantized and an unquantized tensor, we take its unquantized output and we quantize it.  \n",
    "The difference is obvious once we look at the output `bit_width`. In the previous case, we had that the `bit_width` reflected the size of the output accumulator. In this case instead, we have `bit_width=tensor(8.)`, which is what we expected since `output_quant` had been set to an *Int8* quantizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Quantization\n",
    "\n",
    "There is an important scenario where the various options we just saw make a practical difference, and it's quantization of *bias*. In many contexts, such as in the ONNX standard opset and in FINN, bias is assumed to be quantized with scale factor equal to *input scale * weight scale*, which means that we need a valid quantized input somehow. A predefined bias quantizer that reflects that assumption is `brevitas.quant.scaled_int.Int8Bias`. If we simply tried to set it to a `QuantConv2d` without any sort of input quantization, we would get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "QuantLayer is not correctly configured",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb Cell 46\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-host/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbrevitas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquant\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscaled_int\u001b[39;00m \u001b[39mimport\u001b[39;00m Int8Bias\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-host/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m bias_quant_conv \u001b[39m=\u001b[39m QuantConv2d(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-host/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     in_channels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, out_channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m), bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-host/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     bias_quant\u001b[39m=\u001b[39mInt8Bias, return_quant_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bremote-host/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m bias_quant_conv(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m5\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/git/brevitas/src/brevitas/nn/quant_conv.py:198\u001b[0m, in \u001b[0;36mQuantConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Union[Tensor, QuantTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, QuantTensor]:\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_impl(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/git/brevitas/src/brevitas/nn/quant_layer.py:320\u001b[0m, in \u001b[0;36mQuantWeightBiasInputOutputLayer.forward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    316\u001b[0m compute_output_quant_tensor \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(quant_input, QuantTensor) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    317\u001b[0m     quant_weight, QuantTensor)\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (compute_output_quant_tensor \u001b[39mor\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_output_quant_enabled) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_quant_tensor:\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mQuantLayer is not correctly configured\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_quant_tensor \u001b[39mor\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_bias_quant_enabled \u001b[39mand\u001b[39;00m\n\u001b[1;32m    324\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_quant\u001b[39m.\u001b[39mrequires_input_scale \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_quant\u001b[39m.\u001b[39mrequires_input_bit_width))):\n\u001b[1;32m    325\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(quant_input, QuantTensor) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(quant_weight, QuantTensor):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: QuantLayer is not correctly configured"
     ]
    }
   ],
   "source": [
    "from brevitas.quant.scaled_int import Int8Bias\n",
    "\n",
    "bias_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
    "    bias_quant=Int8Bias, return_quant_tensor=True)\n",
    "bias_quant_conv(torch.randn(1, 2, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve the issue by passing in a valid `QuantTensor`, e.g. the `quant_tensor_input`  we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.0058,  0.0030,  0.0030],\n",
       "          [-0.0013, -0.0001,  0.0043],\n",
       "          [-0.0061,  0.0033, -0.0001]],\n",
       "\n",
       "         [[ 0.0013, -0.0008, -0.0015],\n",
       "          [ 0.0011,  0.0012, -0.0012],\n",
       "          [-0.0013, -0.0020,  0.0002]],\n",
       "\n",
       "         [[-0.0061,  0.0053, -0.0004],\n",
       "          [ 0.0028,  0.0031, -0.0038],\n",
       "          [ 0.0026, -0.0048, -0.0044]]]], grad_fn=<ConvolutionBackward0>), scale=tensor([[[[1.8528e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(22.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_quant_conv(quant_tensor_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by enabling input quantization and then passing in a float a `torch.Tensor` or a `QuantTensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.4300,  0.1726, -0.3396],\n",
       "          [ 0.0307, -0.0052, -1.1685],\n",
       "          [-0.3160,  0.1334, -0.4459]],\n",
       "\n",
       "         [[ 1.0135,  0.7129, -0.3874],\n",
       "          [ 0.4858, -0.6205,  0.1563],\n",
       "          [-0.1631, -0.2198,  0.1444]],\n",
       "\n",
       "         [[-1.4600,  0.9106,  0.6328],\n",
       "          [ 0.6669, -0.1814, -0.0169],\n",
       "          [ 0.6581,  0.7420, -0.4884]]]], grad_fn=<ConvolutionBackward0>), scale=tensor([[[[2.9050e-05]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(22.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_bias_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
    "    input_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias, return_quant_tensor=True)\n",
    "input_bias_quant_conv(torch.randn(1, 2, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.0015, -0.0035,  0.0003],\n",
       "          [-0.0054,  0.0047,  0.0055],\n",
       "          [ 0.0043,  0.0054, -0.0050]],\n",
       "\n",
       "         [[-0.0004,  0.0013, -0.0018],\n",
       "          [ 0.0055, -0.0073,  0.0023],\n",
       "          [-0.0053,  0.0009,  0.0032]],\n",
       "\n",
       "         [[ 0.0015, -0.0002, -0.0068],\n",
       "          [ 0.0015, -0.0040, -0.0046],\n",
       "          [-0.0033, -0.0009,  0.0079]]]], grad_fn=<ConvolutionBackward0>), scale=tensor([[[[1.7377e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(22.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_bias_quant_conv(quant_tensor_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the output `bit_width=tensor(22.)`. This is because, in the worst-case, summing a *21-bit* integer (the size of the accumulator before bias is added) and an *8-bit* integer (the size of quantized bias) gives a *22-bit* integer.\n",
    "\n",
    "Let's try now to enable output quantization instead of input quantization. That wouldn't have solved the problem with bias quantization, as output quantization is performed after bias is added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input scale required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb Cell 53\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-host/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m output_bias_quant_conv \u001b[39m=\u001b[39m QuantConv2d(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-host/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     in_channels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, out_channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m), bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-host/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     output_quant\u001b[39m=\u001b[39mInt8ActPerTensorFloat, bias_quant\u001b[39m=\u001b[39mInt8Bias, return_quant_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bremote-host/home/giuseppe/Documents/git/brevitas/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m output_bias_quant_conv(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m5\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/git/brevitas/src/brevitas/nn/quant_conv.py:198\u001b[0m, in \u001b[0;36mQuantConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Union[Tensor, QuantTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, QuantTensor]:\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_impl(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/git/brevitas/src/brevitas/nn/quant_layer.py:334\u001b[0m, in \u001b[0;36mQuantWeightBiasInputOutputLayer.forward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    331\u001b[0m         output_signed \u001b[39m=\u001b[39m quant_input\u001b[39m.\u001b[39msigned \u001b[39mor\u001b[39;00m quant_weight\u001b[39m.\u001b[39msigned\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m     quant_bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_quant(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, output_scale, output_bit_width)\n\u001b[1;32m    335\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_inference_quant_bias \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(quant_bias,\n\u001b[1;32m    336\u001b[0m                                                                             QuantTensor):\n\u001b[1;32m    337\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_bias \u001b[39m=\u001b[39m _CachedIO(quant_bias\u001b[39m.\u001b[39mdetach(), metadata_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/git/brevitas/src/brevitas/proxy/parameter_quant.py:206\u001b[0m, in \u001b[0;36mBiasQuantProxyFromInjector.forward\u001b[0;34m(self, x, input_scale, input_bit_width)\u001b[0m\n\u001b[1;32m    204\u001b[0m impl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexport_handler \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexport_mode \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor_quant\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_input_scale \u001b[39mand\u001b[39;00m input_scale \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput scale required\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_input_bit_width \u001b[39mand\u001b[39;00m input_bit_width \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput bit-width required\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input scale required"
     ]
    }
   ],
   "source": [
    "output_bias_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
    "    output_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias, return_quant_tensor=True)\n",
    "output_bias_quant_conv(torch.randn(1, 2, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all scenarios require bias quantization to depend on the scale factor of the input. In those cases, biases can be quantized the same way weights are quantized, and have their own scale factor. In Brevitas, a predefined quantizer that reflects this other scenario is `Int8BiasPerTensorFloatInternalScaling`. In this case then a valid quantized input is not required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6938,  0.0069,  0.1652],\n",
       "          [-0.4801, -0.8120,  0.5233],\n",
       "          [ 0.4159,  0.4662,  0.2565]],\n",
       "\n",
       "         [[ 0.3206, -0.5500, -0.5254],\n",
       "          [ 0.1864,  1.0210, -0.3706],\n",
       "          [-0.1159,  0.6967, -0.0437]],\n",
       "\n",
       "         [[-0.6209, -0.5257, -0.6592],\n",
       "          [ 0.6389,  0.2658,  0.4542],\n",
       "          [-0.3761, -0.7776, -0.2897]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.quant.scaled_int import Int8BiasPerTensorFloatInternalScaling\n",
    "\n",
    "bias_internal_scale_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
    "    bias_quant=Int8BiasPerTensorFloatInternalScaling, return_quant_tensor=False)\n",
    "bias_internal_scale_quant_conv(torch.randn(1, 2, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of situations to be aware of concerning bias quantization that can lead to changes in the output `zero_point`.\n",
    "\n",
    "Let's consider the scenario where we compute the convolution between a quantized input tensor and quantized weights. In the first case, we then add an *unquantized* bias on top of the output. In the second one, we add a bias quantized with its own scale factor, e.g. with the `Int8BiasPerTensorFloatInternalScaling` quantizer. In both cases, in order to make sure the output `QuantTensor` is valid (i.e. the affine quantization invariant is respected), the output `zero_point` becomes non-zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.4005,  0.7588,  0.4616],\n",
       "          [-0.0777, -0.0651, -0.2405],\n",
       "          [-0.7292,  0.4504,  0.3716]],\n",
       "\n",
       "         [[ 0.4868, -0.4495, -0.1327],\n",
       "          [ 0.2079, -0.3236, -0.5482],\n",
       "          [ 0.5471,  0.1503,  0.6813]],\n",
       "\n",
       "         [[ 0.4356, -0.2319,  1.0867],\n",
       "          [ 0.0126,  0.7646,  0.3627],\n",
       "          [-0.4466,  0.5150,  0.1176]]]], grad_fn=<ConvolutionBackward0>), scale=tensor([[[[2.7130e-05]]]], grad_fn=<MulBackward0>), zero_point=tensor([[[[ 6313.4204]],\n",
       "\n",
       "         [[-2667.2593]],\n",
       "\n",
       "         [[-5507.9629]]]], grad_fn=<DivBackward0>), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unquant_bias_input_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
    "    input_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
    "out_tensor = unquant_bias_input_quant_conv(torch.randn(1, 2, 5, 5))\n",
    "out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out_tensor.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an important point about `QuantTensor`. With the exception of learned bit-width (which will be the subject of a separate tutorial) and some of the bias quantization scenarios we have just seen, usually returing a `QuantTensor` is not necessary and can create extra complexity. This is why currently `return_quant_tensor` defaults to `False`. We can easily see it in an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0650,  0.2496, -1.2857],\n",
       "          [ 1.0231,  0.0516,  0.7592],\n",
       "          [ 0.5882, -0.7619,  0.7604]],\n",
       "\n",
       "         [[-0.6307,  0.1476,  1.0949],\n",
       "          [-0.1488,  0.0472,  0.0097],\n",
       "          [-0.2861,  0.0266, -0.2970]],\n",
       "\n",
       "         [[ 0.0580,  1.2994,  0.3841],\n",
       "          [ 0.2056,  0.0496, -0.7915],\n",
       "          [ 0.4698, -0.8724, -0.0405]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_input_quant_conv = QuantConv2d(\n",
    "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
    "    input_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias)\n",
    "bias_input_quant_conv(torch.randn(1, 2, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough not obvious, the output is actually implicitly quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "98d39f4fefefd06bad749c114549705efc94e31e1348b8fb7b25329049b354fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
