Model loading...
Model loaded.
Data loaded.
Float model eval...
{0: 14637281792.0, 'cpu': 60972624383.99999}
{'': 538060160, 'model': 538060160, 'model.embed_tokens': 113246208, 'model.embed_tokens.weight': 113246208, 'model.layers': 424811520, 'model.layers.0': 14160384, 'model.layers.1': 14160384, 'model.layers.2': 14160384, 'model.layers.3': 14160384, 'model.layers.4': 14160384, 'model.layers.5': 14160384, 'model.layers.6': 14160384, 'model.layers.7': 14160384, 'model.layers.8': 14160384, 'model.layers.9': 14160384, 'model.layers.10': 14160384, 'model.layers.11': 14160384, 'model.layers.12': 14160384, 'model.layers.13': 14160384, 'model.layers.14': 14160384, 'model.layers.15': 14160384, 'model.layers.16': 14160384, 'model.layers.17': 14160384, 'model.layers.18': 14160384, 'model.layers.19': 14160384, 'model.layers.20': 14160384, 'model.layers.21': 14160384, 'model.layers.22': 14160384, 'model.layers.23': 14160384, 'model.layers.24': 14160384, 'model.layers.25': 14160384, 'model.layers.26': 14160384, 'model.layers.27': 14160384, 'model.layers.28': 14160384, 'model.layers.29': 14160384, 'model.norm': 2304, 'model.norm.weight': 2304, 'model.rotary_emb': 128, 'model.rotary_emb.inv_freq': 128}
Float perplexity (wikitext2): 13.739
Applying model quantization...
Model quantization applied.
{0: 14637281792.0, 'cpu': 60972816486.399994}
{'': 656250032, 'model': 543003824, 'model.embed_tokens': 114573312, 'model.embed_tokens.parametrizations': 114573312, 'model.layers': 428428080, 'model.layers.0': 14280936, 'model.layers.1': 14280936, 'model.layers.2': 14280936, 'model.layers.3': 14280936, 'model.layers.4': 14280936, 'model.layers.5': 14280936, 'model.layers.6': 14280936, 'model.layers.7': 14280936, 'model.layers.8': 14280936, 'model.layers.9': 14280936, 'model.layers.10': 14280936, 'model.layers.11': 14280936, 'model.layers.12': 14280936, 'model.layers.13': 14280936, 'model.layers.14': 14280936, 'model.layers.15': 14280936, 'model.layers.16': 14280936, 'model.layers.17': 14280936, 'model.layers.18': 14280936, 'model.layers.19': 14280936, 'model.layers.20': 14280936, 'model.layers.21': 14280936, 'model.layers.22': 14280936, 'model.layers.23': 14280936, 'model.layers.24': 14280936, 'model.layers.25': 14280936, 'model.layers.26': 14280936, 'model.layers.27': 14280936, 'model.layers.28': 14280936, 'model.layers.29': 14280936, 'model.norm': 2304, 'model.norm.weight': 2304, 'lm_head': 113246208, 'lm_head.parametrizations': 113246208, 'lm_head.parametrizations.weight': 113246208, 'model.rotary_emb': 128, 'model.rotary_emb.inv_freq': 128}
{'loss': 6.8676, 'grad_norm': 41.58866882324219, 'learning_rate': 0.09755282581475769, 'epoch': 0.1}
{'loss': 5.7229, 'grad_norm': 26.000600814819336, 'learning_rate': 0.09045084971874738, 'epoch': 0.2}
{'loss': 5.2283, 'grad_norm': 24.903411865234375, 'learning_rate': 0.07938926261462366, 'epoch': 0.3}
{'loss': 5.0972, 'grad_norm': 19.62009620666504, 'learning_rate': 0.06545084971874737, 'epoch': 0.4}
{'loss': 4.9582, 'grad_norm': 20.48581886291504, 'learning_rate': 0.05, 'epoch': 0.5}
{'loss': 4.8559, 'grad_norm': 19.82465362548828, 'learning_rate': 0.03454915028125263, 'epoch': 0.6}
{'loss': 4.7594, 'grad_norm': 16.723188400268555, 'learning_rate': 0.02061073738537635, 'epoch': 0.7}
{'loss': 4.7471, 'grad_norm': 18.374616622924805, 'learning_rate': 0.009549150281252633, 'epoch': 0.8}
{'loss': 4.7286, 'grad_norm': 18.099102020263672, 'learning_rate': 0.0024471741852423235, 'epoch': 0.9}
{'loss': 4.7215, 'grad_norm': 18.88378143310547, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 824.4301, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.121, 'train_loss': 5.168651161193847, 'epoch': 1.0}
{0: 14904458956.8, 'cpu': 60799617535.99999}
{'': 656250032, 'model': 543003824, 'model.embed_tokens': 114573312, 'model.embed_tokens.parametrizations': 114573312, 'model.layers': 428428080, 'model.layers.0': 14280936, 'model.layers.1': 14280936, 'model.layers.2': 14280936, 'model.layers.3': 14280936, 'model.layers.4': 14280936, 'model.layers.5': 14280936, 'model.layers.6': 14280936, 'model.layers.7': 14280936, 'model.layers.8': 14280936, 'model.layers.9': 14280936, 'model.layers.10': 14280936, 'model.layers.11': 14280936, 'model.layers.12': 14280936, 'model.layers.13': 14280936, 'model.layers.14': 14280936, 'model.layers.15': 14280936, 'model.layers.16': 14280936, 'model.layers.17': 14280936, 'model.layers.18': 14280936, 'model.layers.19': 14280936, 'model.layers.20': 14280936, 'model.layers.21': 14280936, 'model.layers.22': 14280936, 'model.layers.23': 14280936, 'model.layers.24': 14280936, 'model.layers.25': 14280936, 'model.layers.26': 14280936, 'model.layers.27': 14280936, 'model.layers.28': 14280936, 'model.layers.29': 14280936, 'model.norm': 2304, 'model.norm.weight': 2304, 'lm_head': 113246208, 'lm_head.parametrizations': 113246208, 'lm_head.parametrizations.weight': 113246208, 'model.rotary_emb': 128, 'model.rotary_emb.inv_freq': 128}
Model eval...
Quantized perplexity (wikitext2): 142.190
Few shot eval results
{'arc_challenge_acc,none': 0.21416382252559726,
 'arc_challenge_acc_norm,none': 0.24914675767918087,
 'arc_challenge_acc_norm_stderr,none': 0.012639407111926505,
 'arc_challenge_acc_stderr,none': 0.011988383205966574,
 'arc_easy_acc,none': 0.2765151515151515,
 'arc_easy_acc_norm,none': 0.2815656565656566,
 'arc_easy_acc_norm_stderr,none': 0.009228934764519166,
 'arc_easy_acc_stderr,none': 0.00917788010146832,
 'piqa_acc,none': 0.5342763873775843,
 'piqa_acc_norm,none': 0.5130576713819369,
 'piqa_acc_norm_stderr,none': 0.011661845375886444,
 'piqa_acc_stderr,none': 0.011638380213532395,
 'winogrande_acc,none': 0.48539857932123126,
 'winogrande_acc_stderr,none': 0.014046492383275955}
