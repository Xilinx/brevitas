# non-defaults only; see default_template.yml for defaults
# adds a BOS token to the beginning of each sequence
bos_preprocessing: sequence
dtype: float32
eval: true
# we use lighteval since it is faster, but using lm_eval 
# requires different strings for few_shot_tasks
few_shot_eval: lighteval
few_shot_override_batch_size: 128
few_shot_tasks:
- lighteval|piqa|0|0
- lighteval|arc:easy|0|0
- leaderboard|arc:challenge|0|0
- leaderboard|winogrande|0|0
- leaderboard|hellaswag|0|0
few_shot_zeroshot: true
# enables ordering by diagonal of H
gpxq_act_order: true
gpxq_block_name: model.layers
# enables activation quantization
input_bit_width: 4
input_quant_granularity: per_row  # per-token
input_scale_type: dynamic
# standardizes RMS norm to the base torch module
replace_rmsnorm: true
# fuses the rotations
rotation: fused_no_fx
# also rotates the scaled dot product attention (SDPA)
rotation_sdpa_regions: true
# specifies Hadamard matrices for rotations
rotation_mode: had
# allows online Hadamard matrices
rotation_orphan_sink: true
model: meta-llama/Llama-3.2-1B
weight_bit_width: 4
# linear search over scaling factor to reduce MSE
weight_param_method: mse
weight_quant_granularity: per_channel
weight_quant_type: sym 
# we use 800 samples from C4 (default) for 
# rotation optimization via CayleySGD
nsamples_rot_calibration: 800
optimize_rotations: true
# using hyperparameters similar to those
# document in the official SpinQuant repo
learning_rate: 1.5
weight_decay: 0.0
lr_scheduler_type: cosine
max_steps: 100
save_safetensors: false
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
logging_steps: 10
log_on_each_node: false
torch_empty_cache_steps: 1
gradient_checkpointing: true