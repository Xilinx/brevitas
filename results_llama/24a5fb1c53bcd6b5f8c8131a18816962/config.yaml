act_calibration: false
act_equalization: null
act_equalization_alpha: 0.5
beta: 0.5
bias_corr: false
checkpoint_name: null
compile_eval: true
compile_ptq: false
config: null
convert_layernorm_to_rmsnorm: false
dataset: wikitext2
dtype: float32
eval: true
export_prefix: null
export_target: null
few_shot_eval: lm_eval
few_shot_limit: null
few_shot_override_batch_size: null
few_shot_tasks:
- arc_challenge
- arc_easy
- winogrande
- piqa
few_shot_zeroshot: false
functional_sdpa_quant: false
fuse_sequences: false
gamma: 0.0
gpfq: false
gptq: false
gpxq_act_order: false
gpxq_block_name: null
gpxq_create_weight_orig: false
gpxq_max_accumulator_bit_width: null
gpxq_max_accumulator_tile_size: null
gpxq_use_quant_activations: false
gradient_accumulation_steps: 8
gradient_checkpointing: true
help: ==SUPPRESS==
input_bit_width: null
input_group_size: 64
input_param_method: stats
input_quant_format: int
input_quant_granularity: per_tensor
input_quant_type: asym
input_scale_precision: float_scale
input_scale_type: static
kv_quant_granularity: null
kv_quant_type: null
learned_round: null
learned_round_fast_update: false
learned_round_iters: 200
learned_round_lr: 0.005
learned_round_scale: false
learned_round_scale_lr: 0.01
learned_round_scale_momentum: 0.9
learning_rate: 0.0001
ln_affine_merge: false
load_awq: null
load_checkpoint: false
log_on_each_node: false
logging_steps: 10
lr_scheduler_type: cosine
max_steps: 100
model: meta-llama/Llama-3.2-1B
no_quantize: false
nsamples: 128
nsamples_rot_calibration: 800
optimize_rotations: true
per_device_train_batch_size: 1
quant_sdpa: false
quantize_input_zero_point: false
quantize_last_layer: false
quantize_weight_zero_point: false
replace_mha: false
replace_rmsnorm: true
rotation: fused_no_fx
rotation_layers_to_expand: []
rotation_mode: had
rotation_orphan_sink: true
rotation_sdpa_regions: true
save_safetensors: false
scale_rounding_func_type: null
scaling_min_val: 0.0001
seed: 0
seqlen: 2048
stiefel_optimizer: riemann_adam
svd_quant: false
svd_quant_iters: 1
svd_quant_rank: 32
temperature: 2.0
use_distillation_loss: true
weight_bit_width: 3
weight_decay: 0.0
weight_equalization: false
weight_group_dim: null
weight_group_size: 128
weight_param_method: stats
weight_quant_format: int
weight_quant_granularity: per_group
weight_quant_type: sym
weight_scale_precision: float_scale
